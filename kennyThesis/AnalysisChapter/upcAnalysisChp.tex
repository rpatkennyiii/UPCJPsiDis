\chapter{Analysis}	
  \section{\label{sec:mcSim} MC Simulation}
    Every physical measurement is the product of the underlaying physics 
      convolved with the response of the detector used to do the measurement. 
    In order to understand the underlaying physical process the detector's 
      effect on the measurement must be understood and accounted for. 
    As instruments become more and more complicated, the interplay between all
      of the many parts of the detector makes an analytic approach to the 
      problem untenable.
    For this reason, the numerical technique of Monte Carlo (MC) simulation is
      the most useful approach.

    MC simulations use random number generation to solve the problem 
      numerically by brute force. 
    First, particles are generated according the theoretical distributions.
    This particles are then propagated through the detector.
    As the particles pass through the detector, random numbers are again used
      to determine how these particles interact with the materials of the 
      detector based on the know properties of the material. 
    In this way, the theoretical distributions are merged with the complex 
      response of the detector. 
    The collective combination of the many sub detectors response with the 
      theoretical distributions emerges from the successive creation of random
      events.
    The result is the convolved response of the detector with the underlaying 
      physical process that is to be studied. 

    In this thesis, two main classes of MC simulation samples were used. 
    The first class uses STARlight to generate events.
    This class of MC samples corresponds to the theoretical calculations 
      described in in Section~\ref{sec:vdmTheory}.
    There are three different physical process described.
    Coherent J/$\psi$ production, where the photon couples to the nucleus as
      a whole, incoherent J/$\psi$ production, where the photon couples to a
      nucleon within the nucleus, and photon-photon process, where the photons
      from the two nuclei interact with each other to produce a lepton pair 
      directly.
    All three STARlight sample produce a $\mu^{+}$ and $\mu^{-}$ in the final 
      state that interacts with the detector.
    The second class uses PYTHIA6 to decay $J/\psi$s with a given
      input $p_{T}$ and rapidity distribution.
    Two samples of this class of particle gun data were produced each with 
      different $p_{T}$ distributions (See Fig.).

    The software chain used for producing the STARlight samples has five steps.
    Because STARlight is not integrated into the standard CMS software 
      framework, this chain was developed for the analysis described in this
      thesis.
    First, STARlight is run in the specified mode, and a single file is 
      created for each physics process, for this thesis, one file for the 
      coherent process, the incoherent process, and the photon-photon process.
    The output from the STARlight generator is in a format specific to 
      STARlight, therefore, the output from the original generation step is 
      then converted to the Les Houches (LHE) format. 
    In this conversion to LHE format, either the parent J/$\psi$ for the 
      J/$\psi$ production samples, or the initial photon-photon pair are added
      LHE output file.
    The standard STARlight output only includes the final state particles.
    Additionally, the initial output from STARlight is split into a collection 
      of smaller LHE files so that each the smaller samples can be 
      processed in parallel.
    Each of the LHE files was then used as input to the CMS software frame work
      (CMSSW).
    The three remaining steps take place within the frame work. 
    First the generated particles are propagated through the GEANT4 detector 
      simulation.
    This accounts for all the interactions with the detector and produces as 
      output a format identical to the raw data that is recorded during data
      taking.
    The next two steps are identical to data taking.
    The reconstruction software used during data taking is run on the output 
      of the detector simulation, and last, the output of the reconstruction
      is reduced to the information that is needed for the final analysis.

    The particle gun samples were created entirely within the CMSSW.
    An interface to PYTHIA6 is included within CMSSW, which takes the J/$\psi$
      $p_{T}$ and rapidity distributions as input. 
    The J/$\psi$ are created according to the input distributions and than 
      PYTHIA6 to $\mu^{+}$ and $\mu^{-}$.
    As with the STARlight samples these muons are propagated through the GEANT4
      simulation of the detector and the raw data is produced.
    The remain steps of running the reconstruction code and reducing the 
      data to the final data needed for the analysis is identical to the 
      STARlight production.

    The five MC samples, three STARlight samples, and two particle gun samples,
      differ primarily in the $p_{T}$ distribution of the J/$\psi$s produced
      and the polarization of the J/$\psi$s, which effects angle at which
      the muon daughters are emitted relative to the direction in which the
      J/$\psi$ is traveling. 
    In Fig.GenPtDist the $p_{T}$ of J/$\psi$ for the coherent and photon-photon
      samples are peaked steeply a low $p_{T}$, and neither sample extends much
      beyond 0.15 GeV in $p_{T}$.
    The incoherent sample is peaked near 0.5 GeV and extends beyond 1 GeV.
    The two particle gun samples resemble the incoherent and coherent samples.
    The first sample has a Gaussian $p_{T}$ distribution extending to 
      approximately 0.15 GeV, whereas the second is flat in $p_{T}$ up to
      2 GeV.
    The particle gun samples are unpolarized, whereas the STARlight samples 
      have transverse polarization. 
    Therefor the particle gun samples there is no preferred direction for the 
      emission of the daughter muons.
    In the STARlight samples however the daughters tend to be emitted in line
      with the direction of the motion of the J/$\psi$.
    This is particularly pronounced for the photon-photon process. 
    
    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=.6\textwidth]{polCosThetaHXGen}
      \caption{ The J/$\psi$ polarization of the \textcolor{red}{particle gun}
        , \textcolor{blue}{coherent}, and incoherent samples are plotted as the
        cosine of the helicity angle.} 
      \label{fig:genHXAngle}
    \end{figure}
  \section{\label{sec:TrigDev} Trigger Development} 
    Prior to the 2011 LHC PbPb run, UPC events had not been directly studied in 
      PbPb collisions using CMS. 
    Design of the UPC triggers required studies of the 2010 data to estimate 
      rates and insure that the bandwidth used by these trigger would be
      sufficiently low. 
    All the different physics analyses must share the limited readout rate of 
      the detector.
    For this reason, conservation of bandwidth was a major design consideration.

    To estimate the 2011 rates prior to the run, the 2010 rates were used to 
      extrapolate to the interaction rate of the 2011 run. 
    The unique UPC triggers were estimated by combining existing triggers from
      the 2010 run. 
    By calculating the ratio between the UPC trigger rates and the minimum bias
      trigger rate, the UPC trigger rates were scaled up to the 2011 
      interaction rates using the 2010 data. 
    The extrapolated rates allowed for a package triggers to be created, which 
      fit within the bandwidth requirement of CMS Heavy Ions. 
    
    The trigger package for 2011 contained ZDC based efficiency monitoring 
      triggers, muon and electron based triggers for measuring $J/\psi$, and 
      backup triggers in case there was a problem with the original muon and 
      electron triggers.
    In order to recorded the trigger efficiency monitoring data, the ZDC 
      triggers had to be prescaled to a lower rate. 
    The scaling down of the monitoring triggers were setup to insure overlap
      with the signal triggers.
    By balancing the competing objectives of rate reduction and increasing 
      the overlap between the monitoring and signal triggers, 
      the prescales for the trigger were as seen in Table .%~\ref{triggerTabel2011}.

    \subsection{\label{sec:l1Trigger} L1 Trigger}
      The goal of the L1 triggers was to record enough data to measure dimuons
        and dielectrons in UPC events.
      To achieve this, the loosest muon trigger and lowest threshold ECAL 
        triggers where paired with a trigger on energy in the ZDC and a veto on
	      energy in the BSC or HF.
      The L1 package that was constructed for the analysis of UPC $J/\psi$ 
        is presented in Table~\ref{tab:l1Triggers2011}.

      \begin{table}[h]
        \centering
        \begin{tabular}{|l|l|}
          L1 Trigger Seed  & Type \\ \hline \hline
          L1\_MuOpen\_ZdcCalo\_NotBscMinBiasThresh2\_BptxAND & Physics \\  \hline
          L1\_EG2\_ZdcCalo\_NotBscMinBiasThresh2\_BptxAND & Physics \\  \hline
          L1\_EG5\_ZdcCalo\_NotBscMinBiasThresh2\_BptxAND & Physics \\ \hline
          L1\_ZdcCaloMinus\_BptxAND & Monitor \\  \hline
          L1\_ZdcCaloMinus\_BptxAND & Monitor \\  \hline
          L1\_MuOpen\_ZdcCalo\_NotHcalHfCoincidencePm\_BptxAND & Backup \\ \hline
          L1\_EG2\_ZdcCalo\_NotHcalHfCoincidencePm\_BptxAND & Backup \\ \hline
          L1\_EG5\_ZdcCalo\_NotHcalHfCoincidencePm\_BptxAND & Backup \\ \hline \hline
        \end{tabular}
        \caption{List of 2011 L1 seeds.}
        \label{tab:l1Triggers2011}
      \end{table}

      The cumulative L1 trigger rate for all the UPC L1 trigger seeds was
        required to be 200 Hz.
      This requirement stemmed from the need to keep the tracker read-out rate
        low. 
      The trackers baseline voltage can fluctuate due to the high tracker hit 
        multiplicities in PbPb collisions.
      In order to monitor the zero suppression of the tracker, the zero 
        suppression algorithm was executed using the HLT computing farm 
	      rather than the in the tracker firmware. 
      The additional computing cycles needed to run the zero suppression 
        the limit for L1 bandwidth. 

    \subsection{HLT Trigger}
      As opposed to the L1 trigger, which reads out the tracker, the HLT has 
        access to the tracker information. 
      Reconstruction of a track in the pixel detector is used by the UPC paths.
      The use of the pixel detector only, as opposed to using the whole tracker 
        including the silicon strip detector, allows for quick track 
	reconstruction saving computing cycles.
      The requirement of at least on reconstructed pixel track for the HLT 
        triggers was designed to reject backgrounds where no particles are 
	reconstructed by the tracker. 
  \begin{table}[h]
		\centering
		\begin{tabular}{|l|l|}
		  \hline HLT Trigger  \\ \hline \hline
		  HLT\_HIUPCNeuMuPixel\_SingleTrack & Physics   \\ \hline
		  HLT\_HIUPCNeuEG2Pixel\_SingleTrack & Physics   \\ \hline
		  HLT\_HIUPCNeuEG5Pixel\_SingleTrack & Physics   \\ \hline
		  HLT\_HIMinBiasZDC\_Calo\_PlusOrMinus\_v1  & Monitor  \\ \hline
		  HLT\_HIMinBiasZDC\_PlusOrMinusPixel\_SingleTrack\_v1   & Monitor \\ \hline
		  HLT\_HIUPCNeuHcalHfMuPixel\_SingleTrack & Backup   \\ \hline
		  HLT\_HIUPCNeuHcalHfEG2Pixel\_SingleTrack & Backup   \\ \hline
		  HLT\_HIUPCNeuHcalHfEG5Pixel\_SingleTrack & Backup   \\ \hline \hline
		\end{tabular}
		\caption{List of 2011 HLT trigger.}
		\label{tab:hltTriggers2011}
	\end{table}

	The total HLT output for the UPC trigger paths was 20 Hz. 
	The limiting factor for the HLT rate was the amount of disk space 
	  available to store the data. 

  \section{\label{sec:DataSetEvSel} Data Sets and Event Selection}
    \subsection{Data Set}
      In order to investigate novel physics processes like UPC $J/\psi$ 
       production, the LHC has delivered unprecedented amounts of data.
      The data for this analysis was recorded during the 2011 LHC PbPb run. 
      During this period, 150 $\mu$$b^{-1}$ where recorded by the CMS detector,
        corresponding to over a billion PbPb collisions. 
      Of this, 143 $\mu$$b^{-1}$ are used in this analysis.
  
      Three specially selected samples were used for the present analysis 
        (see Table~\ref{tab:sampleLumiNevt}).
      These samples were recorded using subsets of the triggers found in 
        Section~\ref{sec:TrigDev}.
      The $J/\psi$ events discussed in this thesis were obtained analyzing the 
      sample labeled in Table~\ref{tab:sampleLumiNevt} as physics.
      A minimum bias sample was recorded for the sake of estimating efficiencies.
      Last, a zero bias sample was recored for investigating the ZDC and the 
        noise distributions of HF.
      By recording this hierarchy of samples, interesting events are selected 
        with a much higher purity in the physics sample, while the zero bias and 
        minimum bias samples allow for the investigation of the selection 
        criteria. 
  
      To record the physics sample containing the $J/\psi$ signal, a muon trigger
        was paired with a veto on energy in the BSC and a requirement that there 
        be energy in at least one of two sides the ZDC. 
      This trigger utilizes the unlikely chance of having overlapping noise in
        in the ZDC and muon detector.
      Because of the characteristically low momentum of UPC $J/\psi$ as compared
        to $J/\psi$ created by other physics process, the loosest muon 
        trigger was used.
      The trigger rejects muon noise by requiring that a interaction took place
        that deposits energy in the ZDC.
      Contributions from hadronic interactions are reduced by the veto on the 
        BSC.
      In this way the balance between reducing the rate and maximizing the 
        efficiency was struck, allowing for the data to be recorded without 
        producing high rates resulting in dead time for the detector.  
      
      In order to investigate the muon trigger and the other parts of the events 
        selection, a minimum bias sample was recorded using the ZDC. 
      For ZDC triggered sample, any event which had energy consistent with at 
        least one neutron in either of the two sides of the ZDC was recorded.
      This process is much more common than the UPC $J/\psi$ production.
      For this reason, the rates of this trigger are much higher than the physics
        trigger, and only a small sub set of these events are recorded.
      From this trigger the pixel track efficiency was estimated. 
  
      In addition to the minimum bias and physics sample, a zero bias sample was 
        recorded to examine the ZDC trigger and the HF noise distributions. 
      The zero bias trigger fired every time both beams passed through CMS. 
      Only 4 events out of every million triggered were recorded for this sample. 
      This sample allowed for an unbiased measurement of the ZDC trigger 
      efficiency as discussed in Section~\ref{sec:effDet}. 
      Because the zero bias trigger does not require any activity in any of the
        CMS sub detectors, the sample contains very few hadronic collisions. 
      This allowed for a measurement of the electronic noise distributions in
        the HF, which will be discussed in the next section.
  
      The integrated luminosity for each of the three samples is calculated
        by recording activity in HF. 
      The cross section for HF activity is measured from a van der Meer scan, and
        the cross section was found to be \textcolor{red}{X}.
      In this way the amount of integrated luminosity for any period running is
        related to the activity in HF. 
      \begin{table}
  	    \centering
  	    \begin{tabular}{|l|l|l|}
  	      \hline Sample & Events & $L_{int}$ \\ \hline \hline
  	      Physics & \textcolor{red}{300K} & \textcolor{red}{143.3 
  	        $\mu$$b$} \\ \hline
  	      Minimum Bias & \textcolor{red}{100K} & \textcolor{red}{X} \\ \hline
  	      Zero Bias & \textcolor{red}{5M} & \textcolor{red}{580 b} \\ \hline \hline
  	    \end{tabular}
  	    \caption{Integrated luminosities and number of events for the three
  	      samples used in this analysis.}
  	    \label{tab:sampleLumiNevt}
      \end{table}
      An additional method was used to cross check the integrated luminosity 
        obtained by the van der Meer scan technique.
      The integrated luminosity can also be measured by counting the events that
        fire the L1 minimum bias trigger together with the inelastic PbPb cross 
        section. 
  
    \subsection{Event selection}
      The analysis described in this thesis focuses on UPC $J/\psi$s decaying to 
        muons. 
      The trigger used for this analysis recored 346841 events.
      A set of off-line cuts were applied to increase the relative contribution 
        of UPC events relative to background processes. 
      The following cuts were applied. 
 
      \begin{table}
        \centering
        \begin{tabular}{|c|c|c|} \hline 
          cut & cut type & events \\ \hline
          all triggered & -- & 346841 \\ \hline
          good vertex requirement & beam background rejection & 340997 \\ \hline
          beam halo muon rejection & beam background rejection & 302777 \\ \hline
          cluster shape compatibility requirement & beam background rejection & 233590 \\ \hline
          single-sided neutron requirement & hadronic interaction rejection & 149992 \\ \hline
          two track requirement & hadronic interaction rejection & 32732 \\ \hline
          HF signal rejection & hadronic interaction rejection & 5392 \\ \hline
          muon quality requirement & fake muon rejection & 1956\\ \hline
          J/$\psi$ mass requirement & kinematic cut & 662 \\ \hline
          muon detectability cuts & kinematic cut & 541 \\ \hline
        \end{tabular}
        \caption{Effects of event selection cuts.}
        \label{tab:evSelCutNumbers}
      \end{table}

      Two sets of event selection cuts were applied to reject background events. 
      The first set rejects background from the beam.
      The second rejects events where hadronic collisions have occurred.
      
      To reject beam induced background the following cuts were applied:
      \begin{itemize}
        \item The reconstructed vertex must be within \textcolor{red}{X} cm in 
          the transverse direction and \textcolor{red}{X} cm in the 
          longitudinal direction. This cut insures that reconstructed particles 
          come from interactions between the two beams rather than event where 
          one of the two beams interact with gas particles near the interaction 
          point. 
  	    \item Beam halo muons were rejected using the timing of the muon hits.
              The beam halo cut rejects events where muons surrounding the beam 
              stream through the detector. 
  	    \item Pixel cluster shape should be compatible with the vertex. 
          This cut requires that energy deposits in the silicon tracker point 
            back to the reconstructed  primary vertex. 
      \end{itemize}
      These beam background cuts do not reject any UPC J/$\psi$ candidates. 
  
      The second set of background rejection cuts were designed to 
        reduce contamination from hadronic interactions. 
      \begin{itemize}
  	    \item No more than 2 reconstructed tracks in the event.
          The track requirement rejects events that produce many charged 
          particles.
  	    \item Maximum reconstructed hit energy in HF was required to be below 
            the threshold for electronic noise. 
          Nearly all hadronic interactions ($\sim$ 98\%) produce particles in 
            the range $3<|\eta|<5$ covered by the HF detector.
          By requiring that the energy deposits in HF resemble noise, nearly all
            elastic hadronic collisions are expected to be rejected.
  	    \item Energy in the ZDCs consistent with neutrons on only one side 
            of the interaction point.
          In hadronic interactions both nuclei break-up. 
          By requiring that ZDC only reconstruct neutrons on one side of the 
            interaction point, hadronic interactions that produce neutrons on 
            both sides were rejected.
      \end{itemize}
      Each of these cuts are designed to reject topologies produced by hadronic
        interactions.
      The effect of these cuts can be seen in Table\textcolor{red}{X}.

      To establish the HF noise thresholds, the noise distributions were 
        measured in zero bias events. 
      Only presences of both beams was required for these events to be recorded. 
      An Off-line selection of events with no reconstructed tracks was used
        to insure that no collision had taken place. 
      The HF noise threshold was defined as the cut that keeps \%99 of the 
        zero bias events.
      The noise distribution from this zero bias sample is compared to the 
        physics sample and MC in Fig.~\ref{fig:hfNoiseDist}.

      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=.6\textwidth]{hfNoiseComp}
        \caption{Comparison of HF noise distributions in zero bias data, 
          physics triggered data, and MC.}
        \label{fig:hfNoiseDist}
      \end{figure}

      The following standard muon quality cuts are applied:
      \begin{itemize}
        \item Tracker track matched with at least one muon segment 
          (in any station) in both X and Y coordinates (< 3 $\sigma$).
        \item Cut on number of tracker layers with hits $>$ 5.
        \item Number of pixel layers $>$ 0.
        \item The $\chi^{2}$ per degrees of freedom of the track fit $<$ 3. 
        \item Loose transverse and longitudinal impact parameter cuts, with in 3 
          cm in the transverse direction and withing 30 cm in the longitudinal 
          direction with respect to the primary vertex.
      \end{itemize}
      These cuts are applied to reduce the number of fake muons.
  
  \section{\label{sec:breakUpDet} Break up determination}
    As described in Section.~\ref{sec:ltaTheory}, UPC J/$\psi$ photoproduction 
      can be accompanied by the emission of neutrons from either of the two 
      colliding nuclei.
    The various neutron emission scenarios, or break-up break up modes, can 
      be distinguished by the ZDC.
    By separating events where the ZDC signal is consistent with 1 neutron 
      versus several neutrons, the different break-up modes can be separated
      and compared to theory. 
    For this reason, reconstruction of the ZDC signal plays an important role 
      in this thesis. 
    In order to maximize the ability to explore the one neutron peak, which 
      sits at the bottom of the ZDCs dynamic range, a new ZDC reconstruction 
      method was devised. 
    This new reconstruction method was than used to establish a one neutron and
      many neutron threshold.
    In this section the ZDC signal reconstruction is described and how the 
      neutron thresholds on this signal were set.
    
    \subsection{ZDC Signal Reconstruction}
      The ZDC signal is built up from the pulse shapes for each of the 
        18 individual ZDC channels. 
      The pulse shape is recorded in 250 ns second chunks and is divided into
        10 time slices of 25 ns (See Fig~\ref{fig:zdcPulseShape}).
      Counting from 0, the 4th time slice is synced with the timing of the rest
        of the detector and corresponds to when the products of the recorded 
        collision reached the ZDC.
      For this reason the channel signal is taken from the 4th time slice.
      \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{zdcPulseShape}
        \caption{ZDC pluse shape.}
        \label{fig:zdcPulseShape}
      \end{figure}

      The ZDC signal sits on top of a low frequency noise pedestal. 
      Over the time scale of 250 ns, this low frequency noise signal appears
        as a constant that shifts randomly from event to event.
      The contribution from this noise is therefore measured event by event
        in order to subtract it.
      Time slice 5 is used for this purpose.

      Time slices 1 and 2 could also be used to estimate the low frequency 
        noise.
      However because the noise fluctuates to negative values of charge that 
        can't be measured by the QIE, these time slice can only provide a 
        measurement half the time. 
      By using time slice 5 which contains the falling tail of the signal, 
        the noise can be measured any time the signal raises significantly 
        above the noise.
      If the fraction of signal in time slice 4 and 5 are constant and
        the noise contributes the same value to both time slices, the 
        following formula is applicable:
      \begin{equation}
        Ts4 \propto (Ts4 + C) - ( Ts5 + C ) = Ts4 - R_{Ts5/Ts4}*Ts4 
        = Ts4(1-R_{Ts5/Ts4}),
        \label{eq:ts4ish}
      \end{equation}
      where $Ts4$ is the signal contribution in time slice 4, $Ts5$ is the 
        signal contribution to time slice 5, $C$ is a random noise constant
        from the low frequency noise, and $R_{Ts5/Ts4}$ is the ratio between
        the signal contribution from time slice 5 over time slice 4.
      Fig.~\ref{fig:zdcTs4OvTs5VTs5} demonstrates the consistence of the 
        fraction and validates the unconventional method of using the falling 
        tail of the signal to estimate the low frequency noise. 
      By using time slice 5 the chances measuring the noise are maximized 
        allowing for the possiblity to sperate the signal from the noise and
        measure the ZDC signal near noise at the bottom of the ZDC dynamic 
        range.
      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.4\textwidth]{negTs5overTs4vts5} &
          \includegraphics[width=.4\textwidth]{posTs5overTs4vts5}
        \end{array} $  
        \caption{ The fraction of signal in time slice 5 over time slice 4 
          as a function of the signal in time slice 5 in ZDC$^{-}$ (left) and 
          ZDC$^{+}$ (right).}
        \label{fig:zdcTs4OvTs5VTs5}
      \end{figure}
      
      To measure one signal value for ZDC$^{+}$ and one for ZDC$^{-}$, the 
        signals from each of the channels are combined.
      Channels from the EM section and HD section are combined first. 
      Only channels with signal above zero in time slice 5 and time slice 
        4 are included. 
      The EM section of the calorimeter is more densely packed with optical 
        fibers and therefore has a higher gain relative to the HAD section. 
      To account for this, the combination of EM channels is weighted with
        a factor of 0.1 to match the HAD channel gains.
      The value for each side of the ZDC's signal is given by the sum of the 
        HAD channel combination and weighted EM channel combination.
      It is this signal, one for ZDC$^{+}$ and one for ZDC$^{-}$, 
        which is plotted to measure the neutron thresholds.

    \subsection{Determination of the one neutron thresholds}
      The ZDC thresholds used to establish the various break-up modes were 
        measured from zero bias data.
      By using this dataset, the spectrum does not contain a trigger bias. 
      The trigger requirement for the zero bias events is that both beams were 
        present in CMS.
      This does however include a significant electronic noise contribution due
        to events where no neutrons are emitted in the direction of the ZDC.

      To determine the thresholds for one and multiple neutrons, the ZDC$^{+}$ 
        and ZDC$^{-}$ spectra are fit.
      Four Gaussian functions were combined to fit the spectra. 
      The electronic noise was fit to a Gaussian about zero.
      The one, two, and three neutron peaks are fit to Gaussians that are 
        successively broader.
      The mean of each peak was initially set to multiples of the mean of the 
        one neutron peak. 
      \begin{figure}[!Hh]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=0.45\textwidth]{zdcFit45Neg} &
          \includegraphics[width=0.45\textwidth]{zdcFit45Pos}
        \end{array} $
        \caption{Fit to the signal spectra for ZDC$^{-}$ (left) and ZDC$^{+}$ 
          (right)}
        \label{fig:zdcM2Fit}
      \end{figure}
      The threshold for a neutron in the ZDC was taken from the fits in 
        Fig.~\ref{fig:zdcM2Fit}.
      Any signal greater 2 $\sigma$ below the mean of the one neutron peak was 
        considered signal.
      Any signal greater than 2 $\sigma$ above was considered multiple 
        neutrons.
      In this way the single neutron break up modes could be separated from the
        multiple neutron modes.

      Several of the break-up mode calculations that have been done involve
        single sided configurations where neutrons are present on one side
        of the interaction point and not the other.
      To identify signal consistent with noise, noise distributions for the 
        combined EM sections and the combined HAD sections were measured.
      The beams are only made to collide every 200 ns. 
      In Fig.~\ref{fig:zdcPulseShape} higher than average signal can be seen
        in the 0th time slice, which precedes the main signal time slice 
        time slice 4 by 200 ns. 
      This is due to events where activity was present in the ZDC for 
        two consecutive collisions.
      Time slices 1 and 2, however, occurred between collisions.
      These time slices were used to estimate the noise spectrum.
      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.45\textwidth]{zdcNegEMNoiseFromZBNoCor} & 
          \includegraphics[width=.45\textwidth]{zdcPosEMNoiseFromZBNoCor} \\
          \includegraphics[width=.45\textwidth]{zdcNegHDNoiseFromZB} &
          \includegraphics[width=.45\textwidth]{zdcPosHDNoiseFromZB}
        \end{array} $
        \caption{ZDC noise spectra from ZDC$^{-}$ EM section (upper left), 
          ZDC$^{+}$ EM section (upper right), ZDC$^{-}$ HAD section (lower left), 
          and ZDC$^{+}$ HAD section (lower right).}
        \label{fig:zdcNoiseSpectra}
      \end{figure}

      Fig~\ref{fig:zdcNoiseSpectra} shows the noise spectrum for each of the 
        EM and HAD sections for the two sides of the ZDC. 
      As with the signal measurements, the low frequency noise pedestal is 
        subtracted event by event by subtracting time slice 2 from time slice
        one before the channel signals are combined for each section.
      A side is considered consistent with noise if both HAD section and EM 
        section signal measurements from the signal method involving time slice
        4 and time slice 5 are lower than 2 sigma below the mean in 
        Fig.~\ref{fig:zdcNoiseSpectra}.
      With the single nuetron, multi-neutron, and noise thresholds established,
        the contributions to the various break-up modes were estimated and 
        compared to theory. 

  \section{\label{sec:sigEx} Signal extraction}
    The invariant mass distribution for opposite sign dimuons is shown in 
      Figure~\ref{fig:massFit}. 
    A J/$\psi$ signal is clearly visible together with tails at higher and lower 
      mass due to the photon-photon process.
    A fit to the invariant mass distribution was done using a Gaussian and a 
      Crystal Ball function to account for the J/psi signal and a first and 
      second order polynomial function for the photon-photon process.
    The extracted number of J/$\psi$ candidates from this fit includes all 
      J/$\psi$s in the mass window that pass the analysis cuts.

    \begin{figure}[!Hhtb]
      \centering
      \includegraphics[width=.6\textwidth]{massFitSimple}
      \caption{Mass fit to J/$\psi$ using Gaussian for the 
        signal and a first order polynomial for the background}
      \label{fig:massFit}
    \end{figure}
   
    The $p_{T}$ spectrum is fit using templates from the three physics MC 
      samples. 
    These contributions display a different shape in transverse momenta. 
    For this reason, the number of coherent J/$\psi$ candidates were extracted 
      from the dimuon $p_{T}$ distribution, around the J/$\psi$ mass, 
      after fitting together MC templates for both signal and background.
    The templates for the fit come form the three STARlight MC samples. 

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=.6\textwidth]{ptOnly}
      \caption{ Fit to MC $p_{T}$ templates. }
      \label{fig:ptTemps}
    \end{figure}

    The shape of the photon-photon process and coherent J/$\psi$ are very 
      similar in $p_{T}$.
    These two shapes normalizations are highly anti-correlated in the fit 
      (see Fig.~\ref{fig:ptOnlyCor}).

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=.6\textwidth]{nCoNGammaCorPtOnly}
      \caption{68\%, 95\%, and 99\% confidence contours from the $p_{T}$ 
        template fit. }
      \label{fig:ptOnlyCor}
    \end{figure}

    To utilize the mass fits ability to distinguish the photon-photon process 
      from the coherent and incoherent process all while utilizing the $p_{T}$
      fits ability to separate the coherent and photon-photon processes from 
      the incoherent, a simultaneous fit to the mass spectrum and $p_{T}$ 
      spectrum was preformed.
    In the simultaneous fit, the parameter for the contribution from the 
      photon-photon process is shared by the two fits.
    
    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=0.9\textwidth]{ptMassSimGaussLine}
      \caption{Simultaneous fit to the mass and $p_{T}$ spectra.}
      \label{fig:simFitMassPtGauss}
    \end{figure}

    By simultaneously fitting both spectra, the anti-correlation between the 
      photon-photon and coherent normalization parameters was reduced as well
      as the overall error on the parameters.
    The slope of the confidence contours in Fig.~\ref{fig:ptOnlyCor} and 
      Fig.~\ref{fig:simGaussCor} demonstrate the reduction in the correlation
      between the normalization parameters. 
    The slope in of the confidence contours in Fig.~\ref{fig:ptOnlyCor} 
      is noticeably closer to 0 than the apparent negative slope in 
      Fig.~\ref{fig:simGaussCor}.
    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=0.6\textwidth]{nCoNGammaCorPtMass}
      \caption{68\%, 95\%, and 99\% confidence contours from the 
        simultaneous fit. }
      \label{fig:simGaussCor}
    \end{figure}

  \section{\label{sec:effDet} Efficiency determination}
    \subsection{Muon Efficiencies}
      The muon efficiencies are measured from MC and data.
      The MC based measurement accounts for the detector acceptance and the 
        efficiency of the muon quality discussed in 
        Section~\ref{sec:DataSetEvSel}.
      The trigger efficiencies were measured in data using the tag and probe 
        method, which is discussed below. 

       CMS has a limited acceptance for $J/\psi$s, particularly in the case of 
        $J/\psi$s with low momentum like those produced in UPC events. 
      To measure the acceptance of CMS for $J/\psi$s, reconstructed dimuon 
        candidates were considered detectable if both reconstructed daughters 
        fell into a detectability region.
      This region was defined using the coherent J/$\psi$ events obtained from 
        STARlight.
      The efficiency for reconstructing single muons $\varepsilon^{\mu}_{reco}$ 
        is defined by $\varepsilon^{\mu}_{reco} = \frac{N^{\mu}_{reco}}{N^{\mu}_{gen}}$, 
        where $N^{\mu}_{reco}$ is the number reconstructed muons after 
        after the full CMS detector simulation and that pass the standard
        muon quality cuts, and $N^{\mu}_{gen}$ is the number of generated 
        muons from STARlight.
      \begin{figure*}[!Hhtb]
        \centering
%          $ \begin{array}{cc}
          \includegraphics[width=.6\textwidth]{mcEffMaps/accMuJpCo} %&
    %      \includegraphics[width=.45\textwidth]{mcEffMaps/accMuJpInCo} \\
    %      \includegraphics[width=.45\textwidth]{mcEffMaps/accMuGamma} &
    %      \includegraphics[width=.45\textwidth]{mcEffMaps/accMuGun}
%         \end{array} $
        \caption{ Muon daughter detectability from coherent J/$\psi$, 
          incoherent J/$\psi$, photon-photon, and J/$\psi$ gun samples.}
        \label{fig:muonDaughterDet}
      \end{figure*}
      Fig.~\ref{fig:muonDaughterDet} shows the efficiency for reconstructing
        single muons from coherent J/$\psi$ events.
      To avoid the edges of the detectors acceptance, all reconstructed muons 
        that fall into a ($p_{T}$,$|\eta|$) bin that has an efficiency less 
        than 20\% were rejected thus defining the detectability region.
      The acceptance for reconstructing dimuons was calculated from MC
        using the following formula:
      \begin{equation}
        A=\frac{N_{det}(|y|,p_{T})}{N_{gen}(|y|,p_{T})},
        \label{eq:jpsiAccEq}
      \end{equation}
        where $N_{det}$ is the number of reconstructed dimuons where both 
        daughters fall in to the detectability region, and $N_{gen}$ is the
        number of generated dimuons. 
      From Eq.~\ref{eq:jpsiAccEq}, the acceptance for $J/\psi$ was calculated
        as a function of $|y|$, and $p_{T}$ (see Fig.~\ref{fig:jpsiAcceptance}).
        \begin{figure*}[!Hhtb]
          \centering
          $ \begin{array}{cc}
            \includegraphics[width=.45\textwidth]{mcEffMaps/detAccJpCoStep} &
            \includegraphics[width=.45\textwidth]{mcEffMaps/detAccJpInCoStep} \\
            \includegraphics[width=.45\textwidth]{mcEffMaps/detAccGammaStep}
          \end{array} $
          \caption{Dimuon acceptance from coherent J/$\psi$ (top left), incoherent 
            J$\psi$ (top right), and photon-photon interactions (lower).}
          \label{fig:jpsiAcceptance}
        \end{figure*}

      The tag and probe method is used to measure the trigger efficiency of 
        the muon daughters, which is a data driven approach. 
      In this method there are three categories of daughter muons. 
      Tag muons are high quality muons.
      Passing probes are reconstructed muons that match the muon trigger, 
        while failing probes do not. 
      Each dimuon will have one daughter classified as a tag and the other
        as a probe.
      From here three invariant mass histograms are studied. 
      One histogram is created from all pairs. 
      The second comes from pairs where the probe is a passing probe.  
      The last histogram comes from pairs where the probe fails to fulfill
        the trigger, \textit{i.e.} the probe is a failing probe. 
      Because this depends on the $p_{T}$ and $|\eta|$ of the probe, one set 
        of three histograms for each ($p_{T}$,$|\eta|$) bin of the probe is 
        created.

      To extract the single muon trigger efficiency $\varepsilon^{\mu}_{trig}$, 
        each set of invariant mass histograms was simultaneously fitted. 
      The signal was fitted using a Crystal Ball function, and the background 
        was fitted to an exponential.
      The Crystal Ball parameters were simultaneously fitted to all three 
        histograms.
      The exponential function was fitted to the failing and passing probe 
        histograms separately.
      Because the background shapes are in principal different for the two 
        samples, the efficiency is driven by this difference. 

      To measure the trigger efficiency a tag is required to pass all muon
        quality cuts and matched to the trigger.
      The probe is required to pass all quality cuts. 
      A passing probe is a probe that is also matched to the trigger. 
      In this way the tag leaves the probe in biased by the trigger and the 
        efficiency can be measued by fitting mass.  

      Fig.~\ref{fig:tnpFitPlot} shows the fit of the three sets of pairs. 
      \begin{figure}[!Hh]
        \centering
        \includegraphics[width=.6\textwidth]{tNp/tnpFits}
        \caption{Fits to tag and probe pairs in the J/$\psi$ mass region.}
        \label{fig:tnpFitPlot}
      \end{figure}
      This fit is done for each bin of the probes $p_{T}$ and $\eta$.
      The resulting fit is in Fig.~\ref{fig:tnpTrigMap}.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=.6\textwidth]{tNp/tnpFromFit}
        \caption{Muon trigger efficiencies in $p_{T}$ and $\eta$ bins from 
          the tag and probe method.}
        \label{fig:tnpTrigMap}
      \end{figure}

      The dimuon trigger efficiency $\varepsilon_{dimuon trigger}$ was measured
        from the single muon efficiencies. 
      The efficiency of each candidate was calculated using the following
        equation:
      \begin{equation}
        \label{eq:dimuTrigEff}
        \varepsilon_{dimuon trigger}=1-(1-\varepsilon_{trigger}^{\mu_{1}})(1-\varepsilon_{trigger}^{\mu_{2}}),
      \end{equation}
      where $\varepsilon_{trigger}^{\mu_{1}}$ is the tag and probe efficiency
        of the first dimuon daughter, and $\varepsilon_{trigger}^{\mu_{2}}$ is
        the efficiency of the second muon daughter. 
      In Eq.~\ref{eq:dimuTrigEff} the probability of at least one daughter
        firing the trigger is calculated by subtracting one from the
        probability that neither daughter fires the trigger,
        thus giving the dimuon trigger efficiency. 

      The average dimuon trigger efficiency for each dimuon ($p_{T}$,$|y|$) bin
        was calculated by averageing the individual dimuon candidates in each
        bin. 
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{averageTriggerEff}
        \caption{The trigger efficiency from tag and probe averaged over candidates
          in each ($p_{T}$,$|y|$) bin.}
        \label{fig:avTrigEffCo}
      \end{figure}
      The average trigger efficiency was multiplied by the acceptance from the MC 
        to produce a total efficiency times acceptance factor. 
      \begin{figure}[!Hhtb]
        \centering
        \includegraphics[width=0.6\textwidth]{averageExA}
        \caption{The acceptance times averaged trigger efficiency from tag and 
          probe.}
        \label{fig:avAccEff}
      \end{figure}

    \subsection{ZDC trigger efficiency}
      A special trigger was prepared to monitor the ZDC trigger efficiency. 
      This trigger required either a ZDC$^{+}$ or ZDC$^{-}$ trigger, together with at 
        least one pixel track. 
      Events were accepted offline if there was no activity in the BSCs or 
        activity on a single side. 
      This sample suffers from a trigger bias. 
      For example, a sample triggered by ZDC$^{+}$ would always produce a ZDC$^{+}$ 
        trigger efficiency of one. 
      To avoid this, the special trigger sample was divided into two 
        subsamples in the following way. 
      A first sample triggered by the ZDC$^{+}$ input and second one triggered by 
        the ZDC$^{-}$. 
      The ZDC$^{+}$ trigger efficiency is measured from the ZDC$^{-}$ sample, and vice 
        versa.

      The trigger efficiency for reconstructed ZDC energies above the
        single neutron threshold were estimated (see for Sec.~\ref{sec:breakUpDet}).
      The ZDC$^{+}$ efficiency was calculated using the ZDC$^{-}$ triggered 
        sample.
      To estimate the efficiency, the number of events with energy in 
        ZDC$^{+}$ greater than the single neutron threshold, N$_{events}$, 
        were measured.
      From this set of events, the number of events that also fire the 
        ZDC$^{+}$ are measured.
      The ratio between the number of single neutron events that fire the 
        trigger and all single neutron events was taken as the estimate of 
        trigger efficiency. 
      The same procedure is applied for each side of the ZDC. 

      \begin{table}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
           ZDC Side & Reco Method & N$_{events}$ & N$_{trig}$ & $\varepsilon_{ZDC}$ \\ \hline
           ZDC$^{+}$ & 1 & 72946  & 71688 & 0.982 $\pm$ 0.005 \\ \hline
           ZDC$^{+}$ & 2 & 73028  & 71706  & 0.9819  $\pm$ 0.005  \\ \hline
           ZDC$^{-}$ & 1 & 76137  & 71786  & 0.9429  $\pm$ 0.005  \\ \hline
           ZDC$^{-}$ & 2 & 76132  & 71859  & 0.9439  $\pm$ 0.005  \\ \hline
        \end{tabular}
        \caption{ZDC trigger efficiencies for ZDC reconstruction method 1 and 
          2}
        \label{tab:zdcEfficiency}
      \end{table}
  \section{\label{sec:sysCheck} Systematic checks}
    \subsection{HF noise threshold}
      The way in which the HF noise distribution is measured effects the event 
        selection.
      In addition, this cut plays a significant role in rejecting hadronic 
        events.
      In Table~\ref{tab:evSelCutNumbers} the importance of cutting on HF noise
        is evident. 
      The HF noise cut rejects \textcolor{red}{X} of the events the remaining. 
      The systematic uncertainties on the HF noise requirement is important for
        this reason.
      The result must not depend significantly on the method used to apply the
        cut on the noise because of the large reduction of events that result
        from it. 
      
      Four different approaches were employed to estimate the systematic effect
        arising from picking a particular method. 
      By looking at the variation of the number of events that remain after 
        applying the thresholds derived from these four methods, the systematic
        uncertainty for the HF noise cut was estimated.
      The four methods are derive from combinations of two variations. 
      The type of object was varied from a low-level detector object called a 
        RecHit to a higher level physics object called a CaloTower. 
      The distinction between the two is that the RecHit is the energy 
        deposited in a single calorimeter detector element, where as the 
        CaloTower is a collection of RecHits with varrious threholds, which 
        represent a full energy deposit that would come from a particle or 
        a collection of particles from a jet passing through the detector. 
      The second variation is on the separation of the two sides.
      In one case the threshold is derived for the two sides combined.
      In another case the thresholds are calculated separately for the two 
        sides of HF.
      By combining these two variations a total of four estimates of the effect
        of the HF noise cut were made.
      Table~\ref{tab:hfNoiseThreshAsym} below shows the thresholds that are 
        measured for each of the four methods.
      The resulting yields from the four different methods are displayed in 
        Table~\ref{tab:hfCutYieldEffects}.

      \begin{table}[!Hhbt]
        \centering
        \begin{tabular}{|c|c|c|c|}
          \hline
          Object type & HF (GeV) & HF$^{-}$ (GeV) & HF$^{+}$ (GeV) \\ \hline
          RecHits & 3.85 & 3.25 & 3.45 \\ \hline
          CaloTowers & 4.25 & 3.25 & 3.75 \\ \hline
        \end{tabular}
        \caption{HF noise theresholds for various noise measurement methods.}
        \label{tab:hfNoiseThreshAsym}
      \end{table}

      \begin{table}[!Hhbt]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Object type & Combinded HF threshold & Two-sided thresholds \\ \hline
          RecHits & 298 & 290 \\ \hline
          CaloTowers & 302 & 288 \\ \hline
        \end{tabular}
        \caption{Candidate yields below 1.05 GeV $p_{T}$ for various HF noise
          cuts.}
        \label{tab:hfCutYieldEffects}
      \end{table}

      The threshold was adjusted to estimate the effect of tightening the
        requirement on the zero bias data.
      By successively lowering the percentage of the zero bias sample
        that was included, the HF noise cut was made more restrictive including
        first 98\%, than 97\% of all zero bias events. 
      This was done for both object types, RecHits and CaloTowers.
      This allows for an estimate of the systematic uncertainty on selecting 
        a 99\% cut.
      Table~\ref{tab:hfAdjustedThresholds} shows the effect on the thresholds
        themselves for both RecHits and CaloToweres, whereas 
        Table~\ref{tab:hfAdjThreshYields} shows the effect on the candidate 
        yields.

      \begin{table}[!Hhbt]
        \begin{center}
          \caption{Values of the energy cuts for the HF calorimeter for RecHit and CaloTower in GeV.}
          \label{tab:hfAdjustedThresholds}
          \begin{tabular}{|c|c|c|} \hline
            \% &  $E_{RecHit}$ GeV & $E_{CaloTower}$ GeV\\ 
            \hline
            99 & 3.85& 4.25 \\ \hline
            98 & 3.25& 3.75 \\ \hline
            97 & 2.95& 3.25 \\  \hline
           \end{tabular}
         \end{center}
      \end{table}

      \begin{table}[!Hhbt]
        \begin{center}
          \caption{Number of dimuon candidates with  p$_{T} <$1.05 when changing HF calorimeter cuts for RecHit and CaloTower.}
          \label{tab:hfAdjThreshYields}
          \begin{tabular}{|c|c|c|} \hline
            \% &  RecHit cut & CaloTower cut\\ \hline
            99 &   298 & 302 \\ \hline
            98 &  287  & 294 \\ \hline
            97 & 284 & 280 \\ \hline
          \end{tabular}
        \end{center}
      \end{table}

    \subsection{MC vs Data compairson}
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{jpsiMcComp/jpsiAbsRapCoherent}
        \caption{Comparison of the of the dimuon rapidity distributions between 
          coherent MC sample and Data.}
        \label{fig:jpsiAbsRapCoherent}
      \end{figure}
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{jpsiMcComp/jpsiPhiCoherent}
        \caption{Comparison of the of the dimuon $\varphi$ distributions 
          between coherent MC sample and Data.}
        \label{fig:jpsiPhiCoherent}
      \end{figure}
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{jpsiMcComp/jpsiPtCoherent}
        \caption{Comparison of the of the dimuon $p_{T}$ distributions 
          between coherent MC sample and Data.}
        \label{fig:jpsiPtCoherent}
      \end{figure}

    \subsection{Tag and probe}
      To estimate the effect the tag and probe fits, the tag and probe 
        efficiencies were additionally measured by counting probes in the 
        J/$\psi$ mass window. 
      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.45\textwidth]{tNp/tnpCounting} &
          \includegraphics[width=.45\textwidth]{tNp/tnpFromFit}
        \end{array} $ 
        \caption{Tag and probe trigger efficiencies from counting (left) 
          compared to fitting (right)}
        \label{fig:tnpCntVFit}
      \end{figure}
    \subsection{Template fit}
      \begin{figure}[!Hhtb]
        \centering
        \includegraphics[width=.6\textwidth]{ptOnly}
        \caption{Coherent, incohernt, and photon-photon process $p_{T}$ template fit to data.}
        \label{fig:ptTempFit}
      \end{figure}

      The normalization for the coherent and incoherent templates are left
        free. 
      To address this issue the template from the photon-photon process was 
        fixed to the cross section value given by STARlight and normalized to
        the integrated luminosity of the data. 
      The normalization for the photon-photon template was allowed a range of 
        20\% due to the 20\% uncertainty in the STARlight cross section.

      The systematic uncertainty for the contribution to the three components
        of the signal was estimated by varying the mass fit.
      Because the mass fit is used to fix the contribution to the $p_{T}$ 
        spectrum of the photon-photon process, the change of the mass fit
        results in different coherent contributions from the $p_{T}$ template
        fit.

      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{ccc}
          \includegraphics[width=.3\textwidth]{cbPolyBkgEst} &
          \includegraphics[width=.3\textwidth]{gausLinBkgEst} &
          \includegraphics[width=.3\textwidth]{gausCCBkgEst} \\
          \includegraphics[width=.3\textwidth]{cbPoly} &
          \includegraphics[width=.3\textwidth]{gausLin} &
          \includegraphics[width=.3\textwidth]{gausCC}
        \end{array} $
        \caption{Various mass distribution fits and the corresponding $p_{T}$
          template fit.}
        \label{fig:massPtFitsForSyst}
      \end{figure}

      Moving from left to right in Fig~\ref{fig:massPtFitsForSyst} the 
        contribution from the photon process increases. 
      The left most fit uses a Crystal Ball function to account for the 
        radiative decay of the final state daughters of the J/$\psi$.
      The low mass exponential portion however picks up background events 
        and overestimates the J/$\psi$ contribution. 
      The right most plot fits the background to a 2nd order Cheby-Chev 
        polyinomial, which overestimates the background.
      Because the Cheby-Chev peaks just below the J/$\psi$ peak, this fit 
        overestimates the background and in turn underestimates the signal 
        contribution.
      The Gaussian fit with a linear background however does a reasonable job
        of fitting both the background and the signal. 

      From these three fits an upper and lower bound of the systematics due
        the choice of fit functions was estimated. 

    \subsection{Mass fit}
      \begin{figure*}[!Hhtb]
        \centering
        \includegraphics[width=.45\textwidth]{massFitSimple}
        \includegraphics[width=.45\textwidth]{massFitCBPoly2}
        \caption{Mass fit to J/$\psi$ using Gaussian (Left) and Crystal Ball (Right) for the 
          signal and a polynomial for the background}
        \label{fig:massFitSys}
      \end{figure*}
      Fig.~\ref{fig:massFitSys} demonstrates the small dependence the raw J/$\psi$ 
        yield has on the fitting function. 
      Both fit functions agree well, with reduced $\chi^{2}$ values below one.
      The Crystal ball fit give an upper estimate for the J/$\psi$ yield.
      The Gaussian fit gives an lower estimate. 
      The main difference comes from the lower mass tails.
      In the Crystal ball fit the lower tail is considered to be signal due to 
        shifting of the mass spectrum to lower mass due to radiation from the 
        final state muons. 
      In the Gaussian fit the lower mass tail is considered to be background and 
        the signal is sharper.

      As check on the simultaneous $p_{T}$ and mass fit, the mass fit is done
        using mass templates from STARlight.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{ptMassSimTemp}
        \caption{Simultaneous fit to the mass and $p_{T}$ using mass templates
          for the mass fit. }
        \label{fig:simFitTemp}
      \end{figure}

    \subsection{MC acceptance}
      To estimate the effect of the GEN level $p_{T}$ distribution on the 
        acceptance the incoherent sample was used to correct the coherent 
        yield.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{coCorInCoAcc}
        \caption{Yields corrected by the MC incoherent acceptance map.}
        \label{fig:coYieldInCoCor}
      \end{figure}

      The effect of polarization was estimated by correcting by the acceptance
        for an polarized J/$\psi$ sample.
      \begin{figure}[!Hhtb]
        \centering
        \includegraphics[width=0.6\textwidth]{coCoGaussGun}
        \caption{Yields corrected by an unpolarized J/$\psi$ sample.}
        \label{fig:coYieldGaussCor}
      \end{figure}

    \subsection{ZDC reconstruction}
      An additional method for estimating the ZDC neutron thresholds was used
        to estimate the systematic errors on the threshold measurements.  
      This additional method used in previous ZDC measurements differs 
        in the way the signal time slices are used to calculate the 
        the signal from each channel.
      In the standard method used in previous analyses, the signal is
        taken from the sum of time slices 4, 5, and 6.
      To estimate the event by event noise pedestal the sum of time slice 
        1 and 2 are used. 
      The signal for an individual ZDC channel is then calculated as the 
        sum of the signal time slices minus the sum of the noise time slices
        weighted by a factor of 3/2 to account for the differing number of 
        noise versus signal time slices.
      In the standard method, the EM section is weighted by a factor of 0.1 
        as in the method described in Section~\ref{sec:breakUpDet}.
      The advantage of the standard method is that by using multiple signal
        and noise time slices the signal and noise are effectively averaged
        reducing time slice to time slice fluctuations.
      However, by using time slices 1 and 2 for measuring the noise, the noise
        can only be measured half the time due to unmeasurable negative 
        fluctuations of the dominant low frequency component of the noise.

      The new method relative to the standard method separates low signal from 
        the noise more effectively for each side of the ZDC as whole.
      This is particularly important for ZDC$^{+}$ where the 1st HAD section
        had a lower gain than the other sections. 
      The ZDC$^{+}$ and ZDC$^{-}$ signals near the one neutron peak using the
        standard and new reconstruction methods were plotted for comparison in 
        Fig.~\ref{fig:zdcSpec2v1}.
      \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{zdcSpec2v1}
        \caption{Comparison of the \textcolor{red}{new} ZDC reconstruction 
          method and the \textcolor{blue}{standard} method for ZDC$^{-}$ (left) and 
          ZDC$^{+}$ (right).}
        \label{fig:zdcSpec2v1}
      \end{figure}
      In Fig.~\ref{fig:zdcSpec2v1}, the shrinking of width of the noise peak 
        around zero in the new method versus the old method is apparent for
        both ZDC$^{+}$ and ZDC$^{-}$.

      Timing cuts were applied to enhance the signal relative to the background
        in order to resolve the one neutron peak in ZDC$^{+}$ using the 
        standard method. 
      Because the products of the collision are synced with time slice 4, noise
        can be rejected by selecting channels where the maximum signal falls 
        into time slice 4.
      The noise will have no preferred time slice (see Fig.~\ref{fig:zdcPulseShape}). 
      Using this fact, signal can be preferably selected by requiring that the
        hadronic channels of the ZDC have a peak signal in the fourth time 
        slice.
      Through these timing cuts the single neutron peak was recovered using the
       standard reconstruction for ZDC$^{+}$.

      To examine the effectiveness of the timing cuts, event by event noise 
        subtraction was removed from the standard reconstruction.
      The signal from each channel was taken from time slices 4,5, and 6 with
        out subtracting 1 and 2.
      The signal spectrum from ZDC$^{-}$ was then plotted with the result
        shown in Fig.~\ref{fig:zdcTimingCuts}.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{zdcMinusSingleNuNoInc}
        \caption{Effects of requiring in-time signal in successively more 
          ZDC hadronic channels, no timing, at least \textcolor{red}{one}, at least \textcolor{green}{two},
            at least \textcolor{blue}{three}, and all \textcolor{yellow}{four} HAD channels have a maximum signal
            in the fourth time slice.}
        \label{fig:zdcTimingCuts}
      \end{figure}
      As each additional hadronic channel is required to have a maximum signal
        in the fourth time slice, the single neutron peak emerges. 
      Fig.~\ref{fig:zdcTimingCuts} demonstrates that the single neutron peak 
        can be recovered from the noise using timing cuts alone. 

      Using the noise subtraction method described by Method 1, the same signal
        emerges.
       \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\textwidth]{zdcMinusSingleNuNoSub}
        \caption{Effect of ZDC signal timing requirements after noise 
          subtraction.}
        \label{fig:zdcTimingAfterNoiseSub}
      \end{figure}
      Fig.~\ref{fig:zdcTimingAfterNoiseSub} confirms that both noise 
        subtraction and the timing require produce the same signal.
      This gives confidence that the signal is not an artifact of either cut, 
        but the true neutron signal.

      The spectrum for method 1 and 2 are fit to a series of Gaussians.
      \begin{figure}[!Hhtb]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=0.45\textwidth]{zdcMinusZBFitTimeCut} &
          \includegraphics[width=0.45\textwidth]{zdcPlusZBFitTimeCut}
        \end{array} $
        \caption{Fit to charge spectrum from ZDC$^{-}$ (left) and ZDC$^{+}$ (right) using 
          method 1}
        \label{fig:zdcM1Fit}
      \end{figure}

       Fig.~\ref{fig:zdcTimingAfterNoiseSub} and Fig.~\ref{fig:zdcSpec2v1} 
        demonstrate the consistence of the using timing cuts and noise 
        subtraction to enhance the signal neutron peak. 
      Fig.~\ref{fig:zdcTimingAfterNoiseSub} confirms the legitimacy of the 
        noise subtraction method in ZDC$^{-}$ by showing the that the same signal
        emerges from the noise subtraction method as the timing method.
      Fig.~\ref{fig:zdcSpec2v1} demonstrates the corresponds between between
        noise subtraction method 1 and method 2 on in ZDC$^{-}$ where signal is 
        better separated from the electronic noise. 
      This allows for confidence that the signal seen in ZDC$^{+}$ using method 2 
        is the one neutron peak.
 
    \subsection{ZDC efficiency}
      To estimate systematic errors in the ZDC trigger efficiency, the trigger
        efficiency was measured from five additional samples. 
      \begin{table}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
          \hline ZDC Side & Reco Method & N$_{events}$ & N$_{trig}$ & $\varepsilon_{ZDC}$ \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{+}$ or ZDC$^{-}$) and 1 pixel track} \\ \hline 
           ZDC$^{+}$ & 1 & 72946  & 71688 & 0.982 $\pm$ 0.005 \\ \hline
           ZDC$^{+}$ & 2 & 73028  & 71706  & 0.9819  $\pm$ 0.005  \\ \hline
           ZDC$^{-}$ & 1 & 76137  & 71786  & 0.9429  $\pm$ 0.005  \\ \hline
           ZDC$^{-}$ & 2 & 76132  & 71859  & 0.9439  $\pm$ 0.005  \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{+}$ or ZDC$^{-}$)} \\ \hline 
           ZDC$^{+}$ & 1 & 30986 & 30333 & 0.9789 $\pm$ 0.0079 \\ \hline
           ZDC$^{+}$ & 2 & 31029 & 30339 & 0.9778 $\pm$ 0.0079 \\ \hline
           ZDC$^{-}$ & 1 & 39178 & 30164 & 0.7699 $\pm$ 0.0059 \\ \hline
           ZDC$^{-}$ & 2 & 35703 & 30443 & 0.8527 $\pm$ 0.0067 \\ \hline
           \multicolumn{5}{|c|}{ Zero Bias} \\ \hline 
           ZDC$^{+}$ & 1 & 109967  & 101598  & 0.9239 $\pm$ 0.0040 \\ \hline
           ZDC$^{+}$ & 2 & 110230  & 101561  & 0.9214 $\pm$ 0.0040 \\ \hline
           ZDC$^{-}$ & 1 & 253241  & 86660  & 0.3422 $\pm$ 0.0013 \\ \hline
           ZDC$^{-}$ & 2 & 156336  & 87401  & 0.5591 $\pm$ 0.0024 \\ \hline
           \multicolumn{5}{|c|}{ Zero Bias with ZDC timing cuts} \\ \hline 
           ZDC$^{+}$ & 1 & 88676  & 84429  & 0.9521 $\pm$ 0.0046 \\ \hline
           ZDC$^{+}$ & 2 & 88480  & 84202  & 0.9517 $\pm$ 0.0046 \\ \hline
           ZDC$^{-}$ & 1 & 59878  & 54728  & 0.9140  $\pm$ 0.0054 \\ \hline
           ZDC$^{-}$ & 2 & 60467  & 54733  & 0.9052  $\pm$ 0.0053 \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{+}$ or ZDC$^{-}$), 1 pixel track, and L1 Muon trigger} \\ \hline 
           ZDC$^{+}$ & 1 & 65466  & 63376  & 0.9681 $\pm$ 0.0054  \\ \hline
           ZDC$^{+}$ & 2 & 65543  & 63358  & 0.9667 $\pm$ 0.0054 \\ \hline
           ZDC$^{-}$ & 1 & 71929  & 63512  & 0.8830  $\pm$ 0.0048 \\ \hline
           ZDC$^{-}$ & 2 & 72932  & 63582  & 0.8718  $\pm$ 0.0047 \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{+}$ or ZDC$^{-}$), 1 pixel track, and L1 EG trigger } \\ \hline 
           ZDC$^{+}$ & 1 & 613758  & 602123  & 0.9810 $\pm$ 0.0018 \\ \hline
           ZDC$^{+}$ & 2 & 614014  & 601863  & 0.9802 $\pm$ 0.0018 \\ \hline
           ZDC$^{-}$ & 1 & 643905  & 602671  & 0.9360  $\pm$ 0.0017 \\ \hline
           ZDC$^{-}$ & 2 & 647888  & 603089  & 0.9309  $\pm$ 0.0017 \\ \hline
        \end{tabular}
        \caption{ZDC trigger efficiencies for ZDC reconstruction method 1 and 
          2 for different trigger samples}
        \label{tab:zdcEfficiencySys}
      \end{table}
      

