\chapter{Analysis}
  In the following chapter the various parts of the measurements done for this
    thesis are explained. 
  The following chapter contains seven sections explain each of these parts: 
    mc simulation, trigger development, data sets and event selection,
    break-up determination, signal extraction, efficiency determination,
    and systematic checks. 
  In Section~\ref{sec:mcSim} the simulations used to estimated the detectors 
    ability to measure UPC processes is discussed. 
  Section~\ref{sec:TrigDev} explains the considerations that went into the 
    triggers which were developed for the analyses discussed in this thesis.
  How the final events were selected and how triggers were used to separate the
    data in to data sets is detailed in Section~\ref{sec:DataSetEvSel}.
  Extraction of the number of events from each of the three physics processes 
    discussed in this thesis, coherent, incoherent, and photon-photon process
    from the final selected events is discussed is Section~\ref{sec:sigEx}.
  The estimates of the detectors efficiency for measuring UPC events is 
    explained in Section~\ref{sec:effDet}.
  Section~\ref{sec:sysCheck} lays out how the systematic uncertainties are 
    estimated.

  \section{\label{sec:mcSim} MC Simulation}
    Every physical measurement is the product of the underlying physics 
      convolved with the response of the detector used to do the measurement. 
    In order to understand the underlying physical process, the detector's 
      effect on the measurement must be understood and accounted for. 
    As instruments become more and more complicated, the interplay between all
      of the many parts of the detector makes an analytic approach to the 
      problem untenable.
    For this reason, the numerical technique of Monte Carlo (MC) simulation is
      the most useful approach.

    MC simulations use random number generation to solve the problem 
      numerically by brute force. 
    First, particles are generated according the theoretical distributions.
    These particles are then propagated through a simulation of the detector.
    As the particles pass through the detector, random numbers are again used
      to determine how these particles interact with the materials of the 
      detector based on the known properties of the material. 
    In this way, the theoretical distributions are merged with the complicated 
      response of the detector. 
    The collective combination of the many sub detectors responses with the 
      theoretical distributions emerges from the successive creation of random
      events.
    The result is the convolved response of the detector with the underlying 
      physical process that is to be studied. 

    In this thesis, two main classes of MC simulation samples were used. 
    The first class uses STARlight to generate events.
    This class of MC samples corresponds to the theoretical calculations 
      described in in Section~\ref{sec:vdmTheory}.
    There are three different physical process described.
    Coherent J/$\psi$ production, where the photon couples to the nucleus as
      a whole, incoherent J/$\psi$ production, where the photon couples to a
      nucleon within the nucleus, and photon-photon process, where the photons
      from the two nuclei interact with each other to produce a lepton pair 
      directly.
    All three STARlight sample produce a $\mu^{+}$ and $\mu^{-}$ in the final 
      state that interacts with the detector.
    The second class uses PYTHIA6 to decay \DIFdelbegin \DIFdel{$J/\psi$}\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$}\DIFaddend s with a given
      input $p_{T}$ and rapidity distribution.
    Two samples of this class of particle gun data were produced each with 
      different $p_{T}$ distributions (See Fig.).

    The software chain used for producing the STARlight samples has five steps.
    Because STARlight is not integrated into the standard CMS software 
      framework (CMSSW), this chain was developed for the analysis described in this
      thesis.
    First, STARlight is run in the specified mode, and a single file is 
      created for each physics process, for this thesis, one file for the 
      coherent process, the incoherent process, and the photon-photon process
      for a total of three files.
    The output from the STARlight generator is in a format specific to 
      STARlight, therefore, the output from the original generation step is 
      then converted to the Les Houches (LHE) format. 
    In this conversion to LHE format, either the parent J/$\psi$ for the 
      J/$\psi$ production samples, or the initial photon-photon pair are added
      to the LHE output file.
    The standard STARlight output only includes the final state particles.
    Additionally, the initial output from STARlight is split into a collection 
      of smaller LHE files so that each of the smaller samples can be 
      processed in parallel.
    Each of the LHE files is used as input to CMSSW.
    The three remaining steps take place within the frame work. 
    First the generated particles are propagated through the GEANT4 detector 
      simulation.
    This accounts for all the interactions with the detector and produces as 
      output a format identical to the raw data that is recorded during data
      taking.
    The next two steps are identical to data taking.
    The reconstruction software used during data taking is run on the output 
      of the detector simulation, and last, the output of the reconstruction
      is reduced to the information that is needed for the final analysis.

    The particle gun samples were created entirely within CMSSW.
    An interface to PYTHIA6 is included within CMSSW, which takes J/$\psi$
      $p_{T}$ and rapidity distributions as input. 
    The J/$\psi$ are created according to the input distributions, and then uses
      PYTHIA6 to decay the J/$\psi$s to $\mu^{+}$ and $\mu^{-}$.
    As with the STARlight samples, these muons are propagated through the GEANT4
      simulation of the detector, and the raw data is produced.
    The remaining steps of running the reconstruction code and reducing the 
      data to the final data needed for the analysis are identical to the 
      STARlight production.

    The five MC samples, three STARlight samples, and two particle gun samples,
      differ primarily in the $p_{T}$ distribution of the J/$\psi$s produced
      and the polarization of the J/$\psi$s, which effects angle at which
      the muon daughters are emitted relative to the direction in which the
      J/$\psi$ is traveling. 
    In Fig.~\ref{fig:starlightRapPtDist} the $p_{T}$ of J/$\psi$s from the 
      coherent and photon-photon samples are peaked steeply a low $p_{T}$, and 
      neither sample extends much beyond 0.15 GeV in $p_{T}$.
    The incoherent sample is peaked near 0.5 GeV and extends beyond 1 GeV.
    The two particle gun samples resemble the incoherent and coherent samples.
    The first sample has a Gaussian $p_{T}$ distribution extending to 
      approximately 0.15 GeV, whereas the second is flat in $p_{T}$ up to
      2 GeV.
    The particle gun samples are unpolarized, whereas the STARlight samples 
      have transverse polarization. 
    Therefor, the particle gun samples there is no preferred direction for the 
      emission of the daughter muons.
    In the STARlight samples however the daughters tend to be emitted in line
      with the direction of the J/$\psi$'s momentum.
    This is particularly pronounced for the photon-photon process.

    \begin{figure}[!Hhbt]
      \centering
      $ \begin{array}{cc}
        \includegraphics[width=0.45\textwidth]{genRapDis} &
        \includegraphics[width=0.45\textwidth]{genPtDis}
      \end{array} $
      \caption{Generator level rapidity (left) and $p_{T}$ (right) 
          distributions for the coherent, \textcolor{red}{incoherent}, 
          and \textcolor{green}{photon-photon} process}
      \label{fig:starlightRapPtDist}
    \end{figure}

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=.6\textwidth]{polCosThetaHXGen}
      \caption{ The J/$\psi$ polarization of the \textcolor{red}{particle gun}
        , \textcolor{blue}{coherent}, and incoherent samples are plotted as the
        cosine of the helicity angle.} 
      \label{fig:genHXAngle}
    \end{figure}

    The momentum of the final state muons is the main drivers of whether the 
      candidate can be measured. 
    The polarization and the $p_{T}$ distribution of dimuons from the generator
      determine the momentum of the daughters. 
    The low $p_{T}$ of the J/$\psi$ restricts the momentum of the $\mu^{+}$ and
      $\mu^{-}$ daughters produced from the J/$\psi$ decay. 
    The polarization effects how the momentum is shared between the daughters.
    In the rest frame of the parent particle from which the daughters decay
      equal momentum is given to each daughter. 
    However in the lab frame of the detector, the muon daughters which are 
      emitted from transversely polarized J/$\psi$ will tend to be emitted in
      the direction of J/$\psi$ and will have unequal momentum in the lab 
      frame.
    The daughter traveling in the direction of the J/$\psi$ will have increased
      momentum, whereas the daughter traveling opposite to the J/$\psi$ 
      direction will have decreased momentum. 
    The combination of these two effects create a muon with very low momentum 
      compared to the typical momenta of muons measured by CMS. 
    The momentum of the lower momentum muon daughters is the main restriction
      on whether or not the J/$\psi$ can be measured. 

  \section{\label{sec:TrigDev} Trigger Development} 
    Prior to the 2011 LHC PbPb run, UPC events had not been directly studied in 
      PbPb collisions using CMS. 
    Design of the UPC triggers required studies of the 2010 data to estimate 
      rates and insure that the bandwidth used by these trigger would be
      sufficiently low. 
    All the different physics analyses must share the limited readout rate of 
      the detector.
    For this reason, conservation of bandwidth was a major design consideration.

    To estimate the 2011 rates prior to the run, the 2010 rates were used to 
      extrapolate to the interaction rate of the 2011 run. 
    The unique UPC triggers were estimated by combining existing triggers from
      the 2010 run. 
    By calculating the ratio between the UPC trigger rates and the minimum bias
      trigger rate, the UPC trigger rates were scaled up to the 2011 
      interaction rates using the 2010 data. 
    The extrapolated rates allowed for a package triggers to be created that 
      fit within the bandwidth requirement of CMS Heavy Ions group. 

    The trigger package for 2011 contained ZDC based efficiency monitoring 
      triggers, muon and electron based triggers for measuring \DIFdelbegin \DIFdel{$J/\psi$}\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$}\DIFaddend , and 
      backup triggers in case there was a problem with the original muon and 
      electron triggers.
    In order to recorded the trigger efficiency monitoring data, the ZDC 
      triggers had to be prescaled to a lower rate. 
    The scaling down of the monitoring triggers were setup to insure overlap
      with the signal triggers.
    By balancing the competing objectives of rate reduction and increasing 
      the overlap between the monitoring and signal triggers, 
      the prescales for the trigger were as seen in Table .%~\ref{triggerTabel2011}.

    \subsection{\label{sec:l1Trigger} L1 Trigger}
      The goal of the L1 triggers was to record enough data to measure dimuons
        and dielectrons in UPC events.
      To achieve this, the loosest muon trigger and lowest threshold ECAL 
        triggers where paired with a trigger on energy in the ZDC and a veto on
	      energy in the BSC.
      Additional triggers which vetoed on energy in HF were commissioned in case
        radiation damaged to the BSCs.
      The L1 package that was constructed for the analysis of UPC \DIFdelbegin \DIFdel{$J/\psi$ 
        }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ 
        }\DIFaddend is presented in Table~\ref{tab:l1Triggers2011}.

      \begin{table}[h]
        \centering
        \begin{tabular}{|l|l|}
          L1 Trigger Seed  & Type \\ \hline \hline
          L1\_MuOpen\_ZdcCalo\_NotBscMinBiasThresh2\_BptxAND & Physics \\  \hline
          L1\_EG2\_ZdcCalo\_NotBscMinBiasThresh2\_BptxAND & Physics \\  \hline
          L1\_EG5\_ZdcCalo\_NotBscMinBiasThresh2\_BptxAND & Physics \\ \hline
          L1\_ZdcCaloMinus\_BptxAND & Monitor \\  \hline
          L1\_ZdcCaloMinus\_BptxAND & Monitor \\  \hline
          L1\_MuOpen\_ZdcCalo\_NotHcalHfCoincidencePm\_BptxAND & Backup \\ \hline
          L1\_EG2\_ZdcCalo\_NotHcalHfCoincidencePm\_BptxAND & Backup \\ \hline
          L1\_EG5\_ZdcCalo\_NotHcalHfCoincidencePm\_BptxAND & Backup \\ \hline \hline
        \end{tabular}
        \caption{List of 2011 L1 seeds.}
        \label{tab:l1Triggers2011}
      \end{table}

      The cumulative L1 trigger rate for all the UPC L1 trigger seeds was
        required to be 200 Hz.
      This requirement stemmed from the need to keep the tracker read-out rate
        low. 
      The trackers baseline voltage can fluctuate due to the high tracker hit 
        multiplicities in PbPb collisions.
      In order to monitor the zero suppression of the tracker, the zero 
        suppression algorithm was executed using the HLT computing farm 
	      rather than the in the tracker firmware.
      The rate at which the tracker could be readout without zero suppression
        set the limit for the L1 bandwidth.


    \subsection{HLT Trigger}
      As opposed to the L1 trigger, which reads out the tracker, the HLT has 
        access to the tracker information. 
      Reconstruction of a track in the pixel detector is used by the UPC paths.
      The use of the pixel detector only, as opposed to using the whole tracker 
        including the silicon strip detector, allows for quick track 
        reconstruction saving computing cycles.
      The requirement of at least one reconstructed pixel track for the HLT 
        triggers was designed to reject backgrounds where no particles are 
        reconstructed by the tracker. 
  \begin{table}[h]
		\centering
		\begin{tabular}{|l|l|}
		  \hline HLT Trigger  \\ \hline \hline
		  HLT\_HIUPCNeuMuPixel\_SingleTrack & Physics   \\ \hline
		  HLT\_HIUPCNeuEG2Pixel\_SingleTrack & Physics   \\ \hline
		  HLT\_HIUPCNeuEG5Pixel\_SingleTrack & Physics   \\ \hline
		  HLT\_HIMinBiasZDC\_Calo\_PlusOrMinus\_v1  & Monitor  \\ \hline
		  HLT\_HIMinBiasZDC\_PlusOrMinusPixel\_SingleTrack\_v1   & Monitor \\ \hline
		  HLT\_HIUPCNeuHcalHfMuPixel\_SingleTrack & Backup   \\ \hline
		  HLT\_HIUPCNeuHcalHfEG2Pixel\_SingleTrack & Backup   \\ \hline
		  HLT\_HIUPCNeuHcalHfEG5Pixel\_SingleTrack & Backup   \\ \hline \hline
		\end{tabular}
		\caption{List of 2011 HLT trigger.}
		\label{tab:hltTriggers2011}
	\end{table}

      The total HLT output for the UPC trigger package was 20 Hz. 
      The limiting factor for the HLT rate was the amount of disk space 
        available to store the data. 
      To meet the bandwidth the requirements and collected a significant sample
        of data for estimating efficiencies, the prescales for the triggers 
        were set. 
      The ZDC trigger that was passed through from the L1 was given a larger 
        prescale to account for the higher rate relative to the more selective 
        ZDC path, which also required a pixel track on the HLT.

  \section{\label{sec:DataSetEvSel} Data Sets and Event Selection}
    In order to investigate novel physics processes like UPC \DIFdelbegin \DIFdel{$J/\psi$ 
     }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ 
     }\DIFaddend production, the LHC has delivered unprecedented amounts of data.
    The data for this analysis was recorded during the 2011 LHC PbPb run. 
    During this period, 150 $\mu$$b^{-1}$ where recorded by the CMS detector,
      corresponding to over a billion PbPb collisions. 
    Of this, 143 $\mu$$b^{-1}$ are used in this analysis.

    \subsection{Data Set}
      Three specially selected samples were used for the present analysis 
        (see Table~\ref{tab:sampleLumiNevt}).
      These samples were recorded using subsets of the triggers found in 
        Section~\ref{sec:TrigDev}.
      The \DIFdelbegin \DIFdel{$J/\psi$ }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ }\DIFaddend events discussed in this thesis were obtained analyzing the 
        sample labeled in Table~\ref{tab:sampleLumiNevt} as physics.
      A ZDC triggered sample was recorded for the sake of estimating 
        efficiencies.
      Last, a zero bias sample was recored for investigating the ZDC and the 
        noise distributions of HF.
      By recording this hierarchy of samples, interesting events are selected 
        with a much higher purity in the physics sample, while the zero bias and 
      ZDC triggered samples allow for the investigation of the selection 
        criteria. 

      To record the physics sample containing the \DIFdelbegin \DIFdel{$J/\psi$ }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ }\DIFaddend signal, a muon trigger
        was paired with a veto on energy in the BSC and a requirement that there 
        be energy in at least one of two sides of the ZDC. 
      This trigger utilizes the unlikely chance of having overlapping noise in
        in the ZDC and muon detector.
      Because of the characteristically low momentum of UPC \DIFdelbegin \DIFdel{$J/\psi$ }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ }\DIFaddend as compared
        to \DIFdelbegin \DIFdel{$J/\psi$ }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ }\DIFaddend created by other physics process, the loosest muon 
        trigger was used.
      By pairing the muon trigger with the ZDC on the L1, the noise contribution
        was reduced from the noise contribution from either of the two 
        sub-detectors to the noise coincidence between the two sub-detectors. 
      Contributions from hadronic interactions are reduced by the veto on the 
        BSC.
      In this way the balance between reducing the rate and maximizing the 
        efficiency was struck, allowing for the data to be recorded without 
        producing high rates resulting in dead time for the detector.  

      In order to investigate the muon trigger and the other parts of the event 
        selection, a minimum bias sample was recorded using the ZDC. 
      For ZDC triggered sample, any event which had energy consistent with at 
        least one neutron in either of the two sides of the ZDC was recorded.
      This process is much more common than the UPC \DIFdelbegin \DIFdel{$J/\psi$ }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ }\DIFaddend production.
      For this reason, the rates of this trigger are much higher than the physics
        trigger, and only a small sub set of these events are recorded.
      From this trigger the pixel track portion of the HLT trigger efficiency 
        was estimated. 

      In addition to the minimum bias and physics sample, a zero bias sample was 
        recorded to examine the ZDC trigger and the HF noise distributions. 
      The zero bias trigger fired every time both beams passed through CMS. 
      Only 4 events out of every million triggered were recorded for this sample. 
      This sample allowed for an unbiased measurement of the ZDC trigger 
      efficiency as discussed in Section~\ref{sec:effDet}. 
      Because the zero bias trigger does not require any activity in any of the
        CMS sub detectors, the sample contains very few hadronic collisions. 
      This allowed for a measurement of the electronic noise distributions in
        the HF, which will be discussed in the next section.

      The integrated luminosity for each of the three samples is calculated
      by recording activity in HF \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{cmsLumi}
}%DIFAUXCMD
}\DIFaddend . 
      The cross section for HF activity is measured from a van der Meer scan, 
        and the cross section was found to be \textcolor{red}{X}.
      In this way the amount of integrated luminosity for any running period is
        related to the activity in HF. 
      \begin{table}
  	    \centering
  	    \begin{tabular}{|l|l|l|}
  	      \hline Sample & Events & $L_{int}$ \\ \hline \hline
  	      Physics & \textcolor{red}{300K} & \textcolor{red}{143.3 
  	        $\mu$$b$} \\ \hline
  	      Minimum Bias & \textcolor{red}{100K} & \textcolor{red}{X} \\ \hline
  	      Zero Bias & \textcolor{red}{5M} & \textcolor{red}{580 b} \\ \hline \hline
  	    \end{tabular}
  	    \caption{Integrated luminosities and number of events for the three
  	      samples used in this analysis.}
  	    \label{tab:sampleLumiNevt}
      \end{table}
      An additional method was used to cross check the integrated luminosity 
        obtained by the van der Meer scan technique.
      The integrated luminosity can also be measured by counting the events that
        fire the L1 minimum bias trigger together with the inelastic PbPb cross 
        section. 

    \subsection{Event selection}
      The analysis described in this thesis focuses on UPC \DIFdelbegin \DIFdel{$J/\psi$}\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$}\DIFaddend s decaying to 
        muons. 
      The trigger used for this analysis recored 346841 events.
      A set of off-line cuts were applied to increase the relative contribution 
        of UPC events to background processes. 
      The following cuts were applied. 

      \begin{table}
        \centering
        \begin{tabular}{|c|c|c|} \hline 
          cut & cut type & events \\ \hline
          all triggered & -- & 346841 \\ \hline
          good vertex requirement & beam background rejection & 340997 \\ \hline
          beam halo muon rejection & beam background rejection & 302777 \\ \hline
          cluster shape compatibility requirement & beam background rejection & 233590 \\ \hline
          single-sided neutron requirement & hadronic interaction rejection & 149992 \\ \hline
          two track requirement & hadronic interaction rejection & 32732 \\ \hline
          HF signal rejection & hadronic interaction rejection & 5392 \\ \hline
          muon quality requirement & fake muon rejection & 1956\\ \hline
          J/$\psi$ mass requirement & kinematic cut & 662 \\ \hline
          muon detectability cuts & kinematic cut & 541 \\ \hline
        \end{tabular}
        \caption{Effects of event selection cuts.}
        \label{tab:evSelCutNumbers}
      \end{table}

      Two sets of event selection cuts were applied to reject background events. 
      The first set rejects background from the beam.
      The second rejects events where hadronic collisions have occurred.

      To reject beam induced background the following cuts were applied:
      \begin{itemize}
        \item The reconstructed vertex must be within \textcolor{red}{X} cm in 
          the transverse direction and \textcolor{red}{X} cm in the 
          longitudinal direction. This cut insures that reconstructed particles 
          come from interactions between the two beams rather than event where 
          one of the two beams interact with gas particles near the interaction 
          point. 
  	    \item Beam halo muons were rejected using the timing of the muon hits.
              The beam halo cut rejects events where muons surrounding the beam 
              stream through the detector. 
  	    \item Pixel cluster shape should be compatible with the vertex. 
          This cut requires that energy deposits in the silicon tracker point 
            back to the reconstructed  primary vertex. 
      \end{itemize}
      These beam background cuts do not reject any UPC J/$\psi$ candidates. 

      The second set of background rejection cuts were designed to 
        reduce contamination from hadronic interactions. 
      \begin{itemize}
  	    \item No more than 2 reconstructed tracks in the event.
          The track requirement rejects events that produce many charged 
          particles.
  	    \item Maximum reconstructed hit energy in HF was required to be below 
            the threshold for electronic noise. 
          Nearly all hadronic interactions ($\sim$ 98\%) produce particles in 
            the range $3<|\eta|<5$ covered by the HF detector.
          By requiring that the energy deposits in HF resemble noise, nearly all
            elastic hadronic collisions are expected to be rejected.
  	    \item Energy in the ZDCs consistent with neutrons on only one side 
            of the interaction point.
          In hadronic interactions both nuclei break-up. 
          By requiring that ZDC only reconstruct neutrons on one side of the 
            interaction point, hadronic interactions that produce neutrons on 
            both sides were rejected.
      \end{itemize}
      Each of these cuts are designed to reject topologies produced by hadronic
        interactions.
      The effect of these cuts can be seen in Table\textcolor{red}{X}.

      To establish the HF noise thresholds, the noise distributions were 
        measured in zero bias events. 
      Only presences of both beams was required for these events to be recorded. 
      An Off-line selection of events with no reconstructed tracks was used
        to insure that no collision had taken place. 
      The HF noise threshold was defined as the cut that keeps \%99 of the 
        zero bias events.
      The noise distribution from this zero bias sample is compared to the 
        physics sample and MC in Fig.~\ref{fig:hfNoiseDist}.

      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=.6\textwidth]{hfNoiseComp}
        \caption{Comparison of HF noise distributions in zero bias data, 
          physics triggered data, and MC.}
        \label{fig:hfNoiseDist}
      \end{figure}

      The following standard muon quality cuts are applied:
      \begin{itemize}
        \item Tracker track matched with at least one muon segment 
          (in any station) in both X and Y coordinates (< 3 $\sigma$).
        \item Cut on number of tracker layers with hits $>$ 5.
        \item Number of pixel layers $>$ 0.
        \item The $\chi^{2}$ per degrees of freedom of the track fit $<$ 3. 
        \item Loose transverse and longitudinal impact parameter cuts, with in 3 
          cm in the transverse direction and withing 30 cm in the longitudinal 
          direction with respect to the primary vertex.
      \end{itemize}
      These cuts are applied to reduce the number of fake muons.

  \section{\label{sec:breakUpDet} Break up determination}
    As described in Section\DIFdelbegin \DIFdel{.}\DIFdelend ~\ref{sec:ltaTheory}, UPC J/$\psi$ photoproduction 
      can be accompanied by the emission of neutrons from either of the two 
      colliding nuclei.
    The various neutron emission scenarios, or break-up \DIFdelbegin \DIFdel{break up }\DIFdelend modes, can 
      be distinguished by the ZDC.
    By separating events where the ZDC signal is consistent with 1 neutron 
      versus several neutrons, the different break-up modes can be separated
      and compared to theory. 
    For this reason, reconstruction of the ZDC signal plays an important role 
      in this thesis. 
    In order to maximize the ability to explore the one neutron peak, which 
      sits at the bottom of the ZDCs dynamic range, a new ZDC reconstruction 
      method was devised. 
    This new reconstruction method was \DIFdelbegin \DIFdel{than }\DIFdelend \DIFaddbegin \DIFadd{then }\DIFaddend used to establish a one neutron and
      many neutron threshold.
    In this section the ZDC signal reconstruction is described and how the 
      neutron thresholds on this signal were set.

    \subsection{ZDC \DIFdelbegin \DIFdel{Signal Reconstruction}\DIFdelend \DIFaddbegin \DIFadd{signal reconstruction}\DIFaddend }
      The ZDC signal is built up from the pulse shapes for each of the 
        18 individual ZDC channels. 
      The pulse shape is recorded in 250 ns second chunks and is divided into
        10 time slices of 25 ns (See Fig~\ref{fig:zdcPulseShape}).
      Counting from 0, the 4th time slice is synced with the timing of the rest
        of the detector and corresponds to when the products of the recorded 
        collision reached the ZDC.
      For this reason the channel signal is taken from the 4th time slice.
      \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{zdcPulseShape}
        \caption{\DIFaddbeginFL \DIFaddFL{Average }\DIFaddendFL ZDC pluse shape \DIFaddbeginFL \DIFaddFL{is plotted as the charge as a function
          of time slice for the first hadronic from ZDC$^{-}$ (left) and 
          ZDC$^{+}$ (right)}\DIFaddendFL .}
        \label{fig:zdcPulseShape}
      \end{figure}

      The ZDC signal sits on top of a low frequency noise pedestal. 
      Over the time scale of 250 ns, this low frequency noise signal appears
        as a constant that shifts randomly from event to event.
      The contribution from this noise is therefore measured event by event
        in order to subtract it.
      Time slice 5 is used for this purpose.

      Time slices 1 and 2 could also be used to estimate the low frequency 
        noise.
      However because the noise fluctuates to negative values of charge that 
        \DIFdelbegin \DIFdel{can't be measuredby the QIE}\DIFdelend \DIFaddbegin \DIFadd{cannot be measured}\DIFaddend , these time \DIFdelbegin \DIFdel{slice }\DIFdelend \DIFaddbegin \DIFadd{slices }\DIFaddend can only provide a 
        measurement \DIFaddbegin \DIFadd{of the noise }\DIFaddend half the time. 
      By using time slice 5 which contains the falling tail of the signal, 
        the noise can be measured any time the signal raises significantly 
        above the noise.
      If the fraction of signal in time slice 4 and 5 are constant and
        the noise contributes the same value to both time slices, the 
        following formula is applicable:
      \begin{equation}
        Ts4 \propto (Ts4 + C) - ( Ts5 + C ) = Ts4 - R_{Ts5/Ts4}\DIFdelbegin \DIFdel{*Ts4 
        }\DIFdelend \DIFaddbegin \DIFadd{Ts4 
        }\DIFaddend = Ts4(1-R_{Ts5/Ts4}),
        \label{eq:ts4ish}
      \end{equation}
      where $Ts4$ is the signal contribution in time slice 4, $Ts5$ is the 
        signal contribution to time slice 5, $C$ is a random noise constant
        from the low frequency noise, and $R_{Ts5/Ts4}$ is the ratio between
        the signal contribution from time slice 5 over time slice 4.
      Fig.~\ref{fig:zdcTs4OvTs5VTs5} demonstrates the consistence of the 
        fraction and validates the unconventional method of using the falling 
        tail of the signal to estimate the low frequency noise. 
      By using time slice 5\DIFdelbegin \DIFdel{the chances }\DIFdelend \DIFaddbegin \DIFadd{, the chances of }\DIFaddend measuring the noise are maximized\DIFdelbegin \DIFdel{allowing for the possiblity to sperate the }\DIFdelend \DIFaddbegin \DIFadd{. 
      Separating the }\DIFaddend signal from the noise \DIFdelbegin \DIFdel{and
        measure }\DIFdelend \DIFaddbegin \DIFadd{is especially important because
        }\DIFaddend the ZDC signal \DIFdelbegin \DIFdel{near }\DIFdelend \DIFaddbegin \DIFadd{for the one neutron peak sits near the }\DIFaddend noise at the 
        bottom of the ZDC dynamic range.
      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.4\textwidth]{negTs5overTs4vts5} &
          \includegraphics[width=.4\textwidth]{posTs5overTs4vts5}
        \end{array} $  
        \caption{ The fraction of signal in time slice 5 over time slice 4 
          as a function of the signal in time slice 5 in ZDC$^{-}$ (left) and 
          ZDC$^{+}$ (right).}
        \label{fig:zdcTs4OvTs5VTs5}
      \end{figure}

      To measure one signal value for ZDC$^{+}$ and one for ZDC$^{-}$, the 
        signals from each of the channels are combined.
      Channels from the EM section and HD section are combined first. 
      Only channels with signal above zero in time slice 5 and time slice 
        4 are included. 
      The EM section of the calorimeter is more densely packed with optical 
        fibers and therefore has a higher gain relative to the HAD section. 
      To account for this, the combination of EM channels is weighted with
        a factor of 0.1 to match the HAD channel gains.
      The value for each side of the ZDC's signal is given by the sum of the 
        HAD channel combination and weighted EM channel combination.
      It is this signal, one for ZDC$^{+}$ and one for ZDC$^{-}$, 
        which is plotted \DIFaddbegin \DIFadd{in Fig.~\ref{fig:zdcM2Fit} }\DIFaddend to measure the neutron 
        thresholds.

    \subsection{Determination of the one neutron thresholds}
      The ZDC thresholds used to establish the various break-up modes were 
        measured from zero bias data.
      By using this dataset, the \DIFaddbegin \DIFadd{neutron }\DIFaddend spectrum does not contain a trigger 
        bias. 
      \DIFdelbegin \DIFdel{The trigger requirement for the zero bias events is }\DIFdelend \DIFaddbegin \DIFadd{Zero bias trigger required }\DIFaddend that both beams were present in CMS.
      This does\DIFdelbegin \DIFdel{however }\DIFdelend \DIFaddbegin \DIFadd{, however, }\DIFaddend include a significant electronic noise contribution due
        to events where no neutrons are emitted in the direction of the ZDC.

      To determine the thresholds for one and multiple neutrons, the ZDC$^{+}$ 
        and ZDC$^{-}$ spectra \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend fit.
      Four Gaussian functions were combined to fit the spectra. 
      The electronic noise was fit to a Gaussian \DIFdelbegin \DIFdel{about }\DIFdelend \DIFaddbegin \DIFadd{around }\DIFaddend zero.
      The one, two, and three neutron peaks are fit to Gaussians that are 
        successively broader.
      The mean of each peak was initially set to multiples of the mean of the 
        one neutron peak. 
      \begin{figure}[!Hh]
        \centering
        $ 
          \begin{array}{cc}
            \includegraphics[width=0.45\textwidth]{zdcFit45Neg} &
            \includegraphics[width=0.45\textwidth]{zdcFit45Pos}
          \end{array} 
        $
        \caption{Fit to the signal spectra for ZDC$^{-}$ (left) and ZDC$^{+}$ 
          (right)}
        \label{fig:zdcM2Fit}
      \end{figure}
      The threshold for a neutron in the ZDC was taken from the fits in 
        Fig.~\ref{fig:zdcM2Fit}.
      Any signal greater 2$\sigma$ below the mean of the one neutron peak was 
        considered signal.
      Any signal greater than 2$\sigma$ above was considered multiple 
        neutrons.
      In this way the single neutron break up modes could be separated from the
        multiple neutron modes.

      Several of the break-up mode calculations that have been done involve
        single sided configurations where neutrons are present on one side
        of the interaction point and not the other.
      To identify signal consistent with noise, noise distributions for the 
        combined EM sections and the combined HAD sections were measured.
      The beams are only made to collide every 200 ns. 
      In Fig.~\ref{fig:zdcPulseShape} higher than average signal can be seen
        in the 0th time slice, which precedes the main signal time slice 
        time slice 4 by 200 ns. 
      This is due to events where activity was present in the ZDC for 
        two consecutive collisions.
      Time slices 1 and 2, however, occurred between collisions.
      These time slices were used to estimate the noise spectrum.
      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.45\textwidth]{zdcNegEMNoiseFromZBNoCor} & 
          \includegraphics[width=.45\textwidth]{zdcPosEMNoiseFromZBNoCor} \\
          \includegraphics[width=.45\textwidth]{zdcNegHDNoiseFromZB} &
          \includegraphics[width=.45\textwidth]{zdcPosHDNoiseFromZB}
        \end{array} $
        \caption{ZDC noise spectra from ZDC$^{-}$ EM section (upper left), 
          ZDC$^{+}$ EM section (upper right), ZDC$^{-}$ HAD section (lower left), 
          and ZDC$^{+}$ HAD section (lower right).}
        \label{fig:zdcNoiseSpectra}
      \end{figure}

      Fig~\ref{fig:zdcNoiseSpectra} shows the noise spectrum for each of the 
        EM and HAD sections for the two sides of the ZDC. 
      As with the signal measurements, the low frequency noise pedestal is 
        subtracted event by event by subtracting time slice 2 from time slice
        one before the channel signals are combined for each section.
      A side is considered consistent with noise if both HAD section and EM 
        section signal measurements from the signal method involving time slice
        4 and time slice 5 are lower than 2 sigma below the mean in 
        Fig.~\ref{fig:zdcNoiseSpectra}.
      With the single \DIFdelbegin \DIFdel{nuetron}\DIFdelend \DIFaddbegin \DIFadd{neutron}\DIFaddend , multi-neutron, and noise thresholds established,
        the contributions to the various break-up modes were estimated and 
        compared to theory. 


  \section{\label{sec:sigEx} Signal extraction}
    After all event selection cuts, the coherent J/$\psi$, incoherent J/$\psi$,
      and photon-photon process all contribute to the remaining events.
    Each process must be separated from the final mix.
    To achieve this, the invariant mass and $p_{T}$ distributions are used 
      to distinguish between the three processes. 
    The photon-photon process is extended in invariant mass whereas the 
      J/$\psi$ is peak strongly near 3.1 GeV.
    In $p_{T}$ the photon-photon and coherent process have similar 
      distributions, both peaked shapely below 0.1 GeV, whereas the incoherent 
      process is more broadly distributed across an interval extending to 
      nearly 1 GeV.
    The mass distribution was \DIFdelbegin \DIFdel{fitted }\DIFdelend \DIFaddbegin \DIFadd{fit }\DIFaddend to separate the photon-photon process from
      the J/$\psi$ process.
    The $p_{T}$ distribution was used to separate the incoherent process from 
      the photon-photon process, and the coherent process. 
    In this way, a separate yield was extracted for all three processes. 

    The invariant mass distribution for opposite sign dimuons is shown in 
      Fig.~\ref{fig:massFit}. 
    A J/$\psi$ signal is clearly visible together with tails at higher and
      \DIFdelbegin \DIFdel{in
      }\DIFdelend lower mass due to the photon-photon process.
    A fit to the invariant mass distribution was done using a Gaussian
      to account for the J/$\psi$ signal and a first order polynomial function 
      for the photon-photon process.
    The extracted number of J/$\psi$ candidates from this fit includes all 
      J/$\psi$s in the mass window that passed the analysis cuts, i.e. both
      coherent and incoherent process contribute to yield from the mass
      fit.
    The $p_{T}$ distribution is needed to separate the two different 
      contributions to the J/$\psi$ peak. 

    \begin{figure}[!Hhtb]
      \centering
      \includegraphics[width=.6\textwidth]{massFitSimple}
      \caption{Mass fit to J/$\psi$ using Gaussian for the 
        signal and a first order polynomial for the photon-photon continuum}
      \label{fig:massFit}
    \end{figure}

    The same candidates from Fig.~\ref{fig:massFit} \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend plotted as a function
      of $p_{T}$ in Fig.~\ref{fig:ptTemps}.
    The clear overlap of the coherent and photon-photon process, and the 
      clear separation of these two lower $p_{T}$ processes from the incoherent
      process \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend apparent.
    The shape of the $p_{T}$ distribution for the coherent, incoherent, and 
      photon-photon process are taken from the final output of MC after
      applying all analysis cuts. 
    To obtain the yields for each of the three process, the $p_{T}$ 
      distribution was fit to the three templates.
    In Fig.\ref{fig:ptTemps}, the yield parameters that were fit were left
      unconstrained for all three process.

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=.6\textwidth]{ptOnly}
      \caption{ Fit to MC $p_{T}$ templates. }
      \label{fig:ptTemps}
    \end{figure}

    The shape of the photon-photon \DIFdelbegin \DIFdel{process }\DIFdelend and coherent J/$\psi$ \DIFaddbegin \DIFadd{process }\DIFaddend are very 
      similar in $p_{T}$.
    Accordingly, the contribution from the photon-photon process and the 
      coherent process are difficult to separate from the $p_{T}$ distribution.
    The confidence contours in Fig.~\ref{fig:ptOnlyCor} from the template fit
      in Fig.~\ref{fig:ptTemps} demonstrate the strong anti-correlation 
      between the coherent yield parameter, $nCo$, and the yield parameter 
      for the photon-photon process, $nGamma$.
    Because of the anti-correlation, the statistical uncertainty on $nCo$ and 
      $nGamma$ from the fit are larger than $\sqrt{nCo}$ and $\sqrt{nGamma}$
      expected from Poisson statistics. 
    The information from the invariant mass and $p_{T}$ distributions \DIFdelbegin \DIFdel{was 
      }\DIFdelend \DIFaddbegin \DIFadd{were
      }\DIFaddend combined to break this correlation. 
    Through this combination, the contribution to the final yield from 
      the three process was measured.

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=.6\textwidth]{nCoNGammaCorPtOnly}
      \caption{68\%, 95\%, and 99\% confidence contours from the $p_{T}$ 
        template fit. }
      \label{fig:ptOnlyCor}
    \end{figure}

    To utilize the mass fits ability to distinguish the photon-photon process 
      from the coherent and incoherent process all while utilizing the $p_{T}$
      fits ability to separate the coherent and photon-photon processes from 
      the incoherent, a simultaneous fit to the mass spectrum and $p_{T}$ 
      spectrum was preformed.
    Fig.~\ref{fig:simFitMassPtGauss} shows the result of the simultaneous fit.
    The simultaneous fit forces the parameter $nGamma$ to both describe the 
      photon-photon continuum present in the side bands of the J/$\psi$ mass 
      peak as well the photon-photon contribution to the low-$p_{T}$ part of 
      the $p_{T}$ spectrum.
    In addition, the J/$\psi$ yield from the mass fit is forced to equal the
      contribution from \DIFdelbegin \DIFdel{from }\DIFdelend the incoherent and coherent process in the 
      fit to the $p_{T}$ distribution. 
    In this way, the correlation between the yield parameters was broken, and 
      the contribution from the three process \DIFdelbegin \DIFdel{are left }\DIFdelend \DIFaddbegin \DIFadd{were made }\DIFaddend independent of each 
      other.

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=0.9\textwidth]{ptMassSimGaussLine}
      \caption{Simultaneous fit to the mass and $p_{T}$ spectra.}
      \label{fig:simFitMassPtGauss}
    \end{figure}

    \DIFdelbegin \DIFdel{By simultaneously fitting both the mass and $p_{T}$ spectra, the yields
      from the coherent, incoherent, and photon-photon process were separated 
      from each other in total yield of final candidates.
    }\DIFdelend The ambiguity between the coherent and incoherent processes in the mass fit
      \DIFaddbegin \DIFadd{and }\DIFaddend the ambiguity between the coherent and the photon-photon process was 
      over come \DIFdelbegin \DIFdel{by combining the two fits}\DIFdelend \DIFaddbegin \DIFadd{through used of the simultaneous fit}\DIFaddend .
    Fig.~\ref{fig:simGaussCor} shows the confidence contours for \DIFdelbegin \DIFdel{nCo and 
      nGamma
      }\DIFdelend \DIFaddbegin \DIFadd{$nCo$ and 
      $nGamma$ }\DIFaddend from the simultaneous fit in Fig.~\ref{fig:simFitMassPtGauss}.  
    The slope of the confidence contours in Fig.~\ref{fig:simGaussCor} 
      is noticeably closer to 0 than the apparent negative slope in 
      Fig.~\ref{fig:ptOnlyCor}.
    The contours for the simultaneous fit are also reduced compared to 
      Fig.~\ref{fig:ptOnlyCor} with widths in $nCo$ and $nGamma$ similar to 
      those expected from Poison statistics. 
    From the simultaneous fit, reasonable statistical errors were obtained 
      along with the yields for the three processes. 

    \begin{figure}[!Hhbt]
      \centering
      \includegraphics[width=0.6\textwidth]{nCoNGammaCorPtMass}
      \caption{68\%, 95\%, and 99\% confidence contours from the 
        simultaneous fit. }
      \label{fig:simGaussCor}
    \end{figure}

  \section{\label{sec:effDet} Efficiency determination}
    \subsection{Muon \DIFdelbegin \DIFdel{Efficiencies}\DIFdelend \DIFaddbegin \DIFadd{efficiencies}\DIFaddend }
      The muon efficiencies are measured from MC and data.
      The MC based measurement accounts for the detector acceptance and the 
        efficiency of the muon quality discussed in 
        Section~\ref{sec:DataSetEvSel}.
      The trigger efficiencies were measured in data using the tag and probe 
      method \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{cmsTnP}
}%DIFAUXCMD
}\DIFaddend , which is discussed below. 

       CMS has a limited acceptance for \DIFdelbegin \DIFdel{$J/\psi$}\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$}\DIFaddend s, particularly in the case of 
        \DIFdelbegin \DIFdel{$J/\psi$}\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$}\DIFaddend s with low momentum like those produced in UPC events. 
      To measure the acceptance of CMS for \DIFdelbegin \DIFdel{$J/\psi$}\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$}\DIFaddend s, reconstructed dimuon 
        candidates were considered detectable if both reconstructed daughters 
        fell into a detectability region.
      This region was defined using the coherent J/$\psi$ events obtained from 
        STARlight.
      The efficiency for reconstructing single muons $\varepsilon^{\mu}_{reco}$ 
        is defined by $\varepsilon^{\mu}_{reco} = \frac{N^{\mu}_{reco}}{N^{\mu}_{gen}}$, 
        where $N^{\mu}_{reco}$ is the number reconstructed muons \DIFdelbegin \DIFdel{after 
        after }\DIFdelend \DIFaddbegin \DIFadd{obtained 
        after }\DIFaddend the full CMS detector simulation and that \DIFdelbegin \DIFdel{pass }\DIFdelend \DIFaddbegin \DIFadd{passed }\DIFaddend the standard
        muon quality cuts, and $N^{\mu}_{gen}$ is the number of generated 
        muons from STARlight.
      \DIFdelbegin %DIFDELCMD < \begin{figure*}[!Hhtb]
%DIFDELCMD <         %%%
\DIFdelend \DIFaddbegin \begin{figure}[!Hhtb]
        \DIFaddendFL \centering
          %DIF <           $ \begin{array}{cc}
          \includegraphics[width=.6\textwidth]{mcEffMaps/accMuJpCo} 
        %DIF < &
    %DIF <       \includegraphics[width=.45\textwidth]{mcEffMaps/accMuJpInCo} \\
    %DIF <       \includegraphics[width=.45\textwidth]{mcEffMaps/accMuGamma} &
    %DIF <       \includegraphics[width=.45\textwidth]{mcEffMaps/accMuGun}
%DIF <          \end{array} $
        \caption{ Muon daughter detectability from coherent J/$\psi$\DIFdelbeginFL \DIFdelFL{, 
          incoherent J/$\psi$, photon-photon, and J/$\psi$ gun samples.}\DIFdelendFL }
        \label{fig:muonDaughterDet}
      \DIFdelbeginFL %DIFDELCMD < \end{figure*}
%DIFDELCMD <       %%%
\DIFdelendFL \DIFaddbeginFL \end{figure}
      \DIFaddend Fig.~\ref{fig:muonDaughterDet} shows the efficiency for reconstructing
        single muons from coherent J/$\psi$ events.
      To avoid the edges of the detectors acceptance, all reconstructed muons 
        that fall into a ($p_{T}$,$|\eta|$) bin that has an efficiency less 
        than 20\% were rejected thus defining the detectability region.
      The acceptance for reconstructing dimuons was calculated from MC
        using the following formula:
      \begin{equation}
        A=\frac{N_{det}(|y|,p_{T})}{N_{gen}(|y|,p_{T})},
        \label{eq:jpsiAccEq}
      \end{equation}
        where $N_{det}$ is the number of reconstructed dimuons where both 
        daughters fall \DIFdelbegin \DIFdel{in to }\DIFdelend \DIFaddbegin \DIFadd{into }\DIFaddend the detectability region, and $N_{gen}$ is the
        number of generated dimuons. 
      From Eq.~\ref{eq:jpsiAccEq}, the acceptance for \DIFdelbegin \DIFdel{$J/\psi$ }\DIFdelend \DIFaddbegin \DIFadd{J/$\psi$ }\DIFaddend was calculated
        as a function of $|y|$, and $p_{T}$ (see Fig.~\ref{fig:jpsiAcceptance}).
        \begin{figure*}[!Hhtb]
          \centering
          $ \begin{array}{cc}
            \includegraphics[width=.45\textwidth]{mcEffMaps/detAccJpCoStep} &
            \includegraphics[width=.45\textwidth]{mcEffMaps/detAccJpInCoStep} \\
            \includegraphics[width=.45\textwidth]{mcEffMaps/detAccGammaStep}
          \end{array} $
          \caption{Dimuon acceptance from coherent J/$\psi$ (top left), incoherent 
            J$\psi$ (top right), and photon-photon interactions (lower).}
          \label{fig:jpsiAcceptance}
        \end{figure*}

      The tag and probe method is used to measure the trigger efficiency of 
        the muon daughters, which is a data driven approach. 
      In this method there are three categories of daughter muons. 
      \DIFdelbegin \DIFdel{Tag muons }\DIFdelend \DIFaddbegin \textit{\DIFadd{Tag muons}} \DIFaddend are high quality muons.
      \DIFdelbegin \DIFdel{Passing probes }\DIFdelend \DIFaddbegin \textit{\DIFadd{Passing probes}} \DIFaddend are reconstructed muons that match the muon trigger, 
        while \DIFdelbegin \DIFdel{failing probes }\DIFdelend \DIFaddbegin \textit{\DIFadd{failing probes}} \DIFaddend do not. 
      Each dimuon will have one daughter classified as a tag and the other
        as a probe.
      From here three invariant mass histograms are studied. 
      One histogram is created from all pairs. 
      The second comes from pairs where the probe is a passing probe.  
      The last histogram comes from pairs where the probe fails to fulfill
        the trigger, \textit{i.e.} the probe is a failing probe. 
      Because this depends on the $p_{T}$ and $|\eta|$ of the probe, one set 
        of three histograms for each ($p_{T}$,$|\eta|$) bin of the probe is 
        created.

      To extract the single muon trigger efficiency $\varepsilon^{\mu}_{trig}$, 
        each set of invariant mass histograms was simultaneously fitted. 
      The signal was fitted using a Crystal Ball function, and the background 
        was fitted to an exponential.
      The Crystal Ball parameters were simultaneously fitted to all three 
        histograms.
      The exponential function was fitted to the failing and passing probe 
        histograms separately.
      Because the background shapes are in \DIFdelbegin \DIFdel{principal }\DIFdelend \DIFaddbegin \DIFadd{principle }\DIFaddend different for the two 
        samples, the efficiency is driven by this difference. 

      To measure the trigger efficiency a tag is required to pass all muon
        quality cuts and matched to the trigger.
      The probe is required to pass all quality cuts. 
      A passing probe is a probe that is also matched to the trigger. 
      In this way\DIFaddbegin \DIFadd{, }\DIFaddend the tag leaves the probe \DIFdelbegin \DIFdel{in biased }\DIFdelend \DIFaddbegin \DIFadd{unbiased }\DIFaddend by the trigger and the 
        efficiency can be \DIFdelbegin \DIFdel{measued by fitting mass}\DIFdelend \DIFaddbegin \DIFadd{measured by fitting the mass distribution}\DIFaddend .  

      Fig.~\ref{fig:tnpFitPlot} shows the fit of the three sets of pairs. 
      \begin{figure}[!Hh]
        \centering
        \includegraphics[width=.6\textwidth]{tNp/tnpFits}
        \caption{Fits to tag and probe pairs in the J/$\psi$ mass region.}
        \label{fig:tnpFitPlot}
      \end{figure}
      This fit is done for each bin of the probes $p_{T}$ and $\eta$.
      The resulting fit is in Fig.~\ref{fig:tnpTrigMap}.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=.6\textwidth]{tNp/tnpFromFit}
        \caption{Muon trigger efficiencies in $p_{T}$ and $\eta$ bins from 
          the tag and probe method.}
        \label{fig:tnpTrigMap}
      \end{figure}

      The dimuon trigger efficiency \DIFdelbegin \DIFdel{$\varepsilon_{dimuon trigger}$ }\DIFdelend \DIFaddbegin \DIFadd{$\varepsilon^{dimuon}_{trigger}$ }\DIFaddend was measured
        from the single muon efficiencies. 
      The efficiency of each candidate was calculated using the following
        equation:
      \begin{equation}
        \label{eq:dimuTrigEff}
        \varepsilon\DIFdelbegin \DIFdel{_{dimuon trigger}}\DIFdelend \DIFaddbegin \DIFadd{^{dimuon}_{trigger}}\DIFaddend =1-(1-\varepsilon_{trigger}^{\mu_{1}})(1-\varepsilon_{trigger}^{\mu_{2}}),
      \end{equation}
      where $\varepsilon_{trigger}^{\mu_{1}}$ is the tag and probe efficiency
        of the first dimuon daughter, and $\varepsilon_{trigger}^{\mu_{2}}$ is
        the efficiency of the second muon daughter. 
      In Eq.~\ref{eq:dimuTrigEff} the probability of at least one daughter
        firing the trigger is calculated by subtracting one from the
        probability that neither daughter fires the trigger,
        thus giving the dimuon trigger efficiency. 

      The average dimuon trigger efficiency for each dimuon ($p_{T}$,$|y|$) bin
        was calculated by averageing the individual dimuon candidates in each
        bin. 
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{averageTriggerEff}
        \caption{The trigger efficiency from tag and probe averaged over candidates
          in each ($p_{T}$,$|y|$) bin.}
        \label{fig:avTrigEffCo}
      \end{figure}
      The average trigger efficiency was multiplied by the acceptance from the MC 
        to produce a total \DIFdelbegin \DIFdel{efficiency times acceptancefactor}\DIFdelend \DIFaddbegin \DIFadd{factor for both efficiency and acceptance}\DIFaddend . 
      \begin{figure}[!Hhtb]
        \centering
        \includegraphics[width=0.6\textwidth]{averageExA}
        \caption{The acceptance times averaged trigger efficiency from tag and 
          probe.}
        \label{fig:avAccEff}
      \end{figure}

      \DIFaddbegin \DIFadd{The total combined efficiency and acceptance factor coherent J/$\psi$ 
        between 2.0 < |y| 2.2 was found to be around 5\%.
      The roughly 7\% acceptance factor from the MC is the main contributor
        to the total efficiency. 
      Primarily, the interplay of the polarization of the J/$\psi$ and
        the material in detector drive down the efficiency by creating an 
        effective momentum threshold for detection (see 
        Section~\ref{sec:mcSim}).
      The reconstruction efficiency of the daughters is ranges between 
        20\%-60\% for muons in the defined detectability range. 
      The trigger efficiency for the detectable muons ranges from 30\%-80\% 
        depending on $p_{T}$. 
      The typical trigger efficiency for the dimuons was ranges from 60\% to
        80\%.
        }


    \DIFaddend \subsection{ZDC trigger efficiency}
      \DIFdelbegin \DIFdel{A }\DIFdelend \DIFaddbegin \DIFadd{As discussed in Section~\ref{sec:breakUpDet}, a }\DIFaddend special trigger was 
        prepared to monitor the ZDC trigger efficiency. 
      This trigger required either a ZDC$^{+}$ or ZDC$^{-}$ trigger, together with at 
        least one pixel track. 
      Events were accepted offline if there was no activity in the BSCs or 
        activity on a single side. 
\DIFaddbegin 

      \DIFaddend This sample suffers from a trigger bias. 
      For example, a sample triggered by ZDC$^{+}$ would always produce a ZDC$^{+}$ 
        trigger efficiency of one. 
      To avoid this, the special trigger sample was divided into two 
        subsamples in the following way. 
      A first sample triggered by the ZDC$^{+}$ input and second one triggered by 
        the ZDC$^{-}$. 
      The ZDC$^{+}$ trigger efficiency is measured from the ZDC$^{-}$ sample, and vice 
        versa.

      The trigger efficiency for reconstructed ZDC energies above the
        single neutron threshold were estimated (see for Sec.~\ref{sec:breakUpDet}).
      The ZDC$^{+}$ efficiency was calculated using the ZDC$^{-}$ triggered 
        sample.
      To estimate the efficiency, the number of events with energy in 
        ZDC$^{+}$ greater than the single neutron threshold, N$_{events}$, 
        were measured.
      From this set of events, the number of events that also fire the 
        ZDC$^{+}$ \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend measured.
      The ratio between the number of single neutron events that \DIFdelbegin \DIFdel{fire }\DIFdelend \DIFaddbegin \DIFadd{fired }\DIFaddend the 
        trigger and all single neutron events was taken as the estimate of 
        trigger efficiency. 
      The same procedure \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend applied for each side of the ZDC.
      \DIFaddbegin \DIFadd{The trigger efficiency of the ZDC was found to be 98\% for ZDC$^{-}$
        and 94\% for ZDC$^{+}$.
}\DIFaddend 

      \begin{table}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
           ZDC Side & Reco Method & N$_{events}$ & N$_{trig}$ & $\varepsilon_{ZDC}$ \\ \hline
           ZDC$^{+}$ & 1 & 72946  & 71688 & 0.982 $\pm$ 0.005 \\ \hline
           ZDC$^{+}$ & 2 & 73028  & 71706  & 0.9819  $\pm$ 0.005  \\ \hline
           ZDC$^{-}$ & 1 & 76137  & 71786  & 0.9429  $\pm$ 0.005  \\ \hline
           ZDC$^{-}$ & 2 & 76132  & 71859  & 0.9439  $\pm$ 0.005  \\ \hline
        \end{tabular}
        \caption{ZDC trigger efficiencies for ZDC reconstruction method 1 and 
          2}
        \label{tab:zdcEfficiency}
      \end{table}
\DIFaddbegin 

  \DIFaddend \section{\label{sec:sysCheck} Systematic checks}

    \DIFaddbegin \DIFadd{Table~\ref{tab:sumsyst} shows the systematic errors that were estimated.
    The method used to separate the coherent from the photon-photon process 
     is the most dominant error.
    The ZDC reconstruction method used to estimate the neutron thresholds 
      is the next most dominant, followed by the method used to estimate
      the HF noise threshold. 
    }

    \DIFaddend \begin{table}[!Hhtb]
      \begin{center}
        \DIFdelbeginFL %DIFDELCMD < \label{tab:sumsyst}
%DIFDELCMD <         %%%
\DIFdelendFL \begin{tabular}{|c|c|c|}
          \hline
          systematic & uncertainty in \%  \\ \hline
          Template fit normalized & +9.5\% -12\%    \\ \hline
          ZDC reconstruction  & 2.9\%  \\ \hline
          ZDC trigger efficiency & 2.2\%    \\ \hline
          HF noise threshold & +1.3\% -3.4\%    \\ 
          MC acceptance & 1.1\%    \\ \hline
          \DIFdelbeginFL \DIFdelFL{Mass fits }%DIFDELCMD < & %%%
\DIFdelFL{1.9\%    }%DIFDELCMD < \\ %%%
\DIFdelendFL %\hline \hline
          \DIFdelbeginFL %DIFDELCMD < \hline
%DIFDELCMD <           %%%
\DIFdelendFL Total systematic & 8.1\%    \\ \hline
        \end{tabular}
        \caption{Summary of systematic uncertainties}
        \DIFaddbeginFL \label{tab:sumsyst}
      \DIFaddendFL \end{center}
    \end{table}

    \subsection{HF noise threshold}
      The way in which the HF noise distribution is measured effects the event 
        selection and therefore the final candidate yeild.
      This cut plays a significant role in rejecting hadronic events.
      In Table~\ref{tab:evSelCutNumbers} the importance of cutting on HF noise
        is evident. 
      The HF noise cut rejects a little less than 1/5 of the remaining events. 
      The systematic uncertainties on the HF noise requirement is important for
        this reason.
      The result must not depend significantly on the method used to apply the
        cut on the noise because of the large reduction of events that result
        from it. 

      Four different approaches were employed to estimate the systematic effect
        arising from picking a particular method for setting the HF noise
        threshold. 
      By looking at the variation of the number of events that remain after 
        applying the thresholds derived from these four methods, the systematic
        uncertainty for the HF noise cut was estimated.
      The four methods are derive from combinations of two variations. 
      The type of object was varied from a low-level detector object called a 
        RecHit to a higher level physics object called a CaloTower. 
      The RecHit is the energy deposited in a single calorimeter detector 
        element, where as the CaloTower is a collection of RecHits with 
        varrious threholds, which represent a full energy deposit that would 
        come from a particle or a collection of particles from a jet passing 
        through the detector. 
      The second variation is on the separation of the two sides.
      In one case the threshold is derived for the two sides combined.
      In another case the thresholds are calculated separately for the two 
        sides of HF.
      By combining these two variations, a total of four estimates of the 
        effect of the HF noise cut were made.
      Table~\ref{tab:hfNoiseThreshAsym} below shows the thresholds that are 
        measured for each of the four methods.
      The resulting yields from the four different methods are displayed in 
        Table~\ref{tab:hfCutYieldEffects}.

      \begin{table}[!Hhbt]
        \centering
        \begin{tabular}{|c|c|c|c|}
          \hline
          Object type & HF (GeV) & HF$^{-}$ (GeV) & HF$^{+}$ (GeV) \\ \hline
          RecHits & 3.85 & 3.25 & 3.45 \\ \hline
          CaloTowers & 4.25 & 3.25 & 3.75 \\ \hline
        \end{tabular}
        \caption{HF noise theresholds for various noise measurement methods.}
        \label{tab:hfNoiseThreshAsym}
      \end{table}

      \begin{table}[!Hhbt]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Object type & Combinded HF threshold & Two-sided thresholds \\ \hline
          RecHits & 298 & 290 \\ \hline
          CaloTowers & 302 & 288 \\ \hline
        \end{tabular}
        \caption{Candidate yields below 1.05 GeV $p_{T}$ for various HF noise
          cuts.}
        \label{tab:hfCutYieldEffects}
      \end{table}

      The threshold was adjusted to estimate the effect of tightening the
        requirement on the zero bias data.
      By successively lowering the percentage of the zero bias sample
        that was included, the HF noise cut was made more restrictive including
        first 98\%, than 97\% of all zero bias events. 
      This was done for both object types, RecHits and CaloTowers.
      This allows for an estimate of the systematic uncertainty on selecting 
        a 99\% cut.
      Table~\ref{tab:hfAdjustedThresholds} shows the effect on the thresholds
        themselves for both RecHits and CaloToweres, whereas 
        Table~\ref{tab:hfAdjThreshYields} shows the effect on the candidate 
        yields.

      \begin{table}[!Hhbt]
        \begin{center}
          \caption{Values of the energy cuts for the HF calorimeter for RecHit and CaloTower in GeV.}
          \label{tab:hfAdjustedThresholds}
          \begin{tabular}{|c|c|c|} \hline
            \% &  $E_{RecHit}$ GeV & $E_{CaloTower}$ GeV\\ 
            \hline
            99 & 3.85& 4.25 \\ \hline
            98 & 3.25& 3.75 \\ \hline
            97 & 2.95& 3.25 \\  \hline
           \end{tabular}
         \end{center}
      \end{table}

      \begin{table}[!Hhbt]
        \begin{center}
          \caption{Number of dimuon candidates with  p$_{T} <$1.05 when changing HF calorimeter cuts for RecHit and CaloTower.}
          \label{tab:hfAdjThreshYields}
          \begin{tabular}{|c|c|c|} \hline
            \% &  RecHit cut & CaloTower cut\\ \hline
            99 &   298 & 302 \\ \hline
            98 &  287  & 294 \\ \hline
            97 & 284 & 280 \\ \hline
          \end{tabular}
        \end{center}
      \end{table}

      The systematic uncertainty in the HF noise threshold measurement was 
        calculated taking the difference from the 99\% combined RecHit method
        with the upper and lower extrema. 
      The systematic uncertainty from this method is calculated to be +1.3\% 
        -3.4\%.


    \subsection{Template fit normalization}
      \begin{figure}[!Hhtb]
        \centering
        \includegraphics[width=.6\textwidth]{ptOnly}
        \caption{Coherent, incoherent, and photon-photon process $p_{T}$ template fit to data.}
        \label{fig:ptTempFit}
      \end{figure}

      The $p_{T}$ template fit depends on the functions chosen for the fit
        to the mass distribution.
      As described in Section~\ref{sec:sigEx}, the similarity of the of the 
        $p_{T}$ distribution for the coherent and photon-photon process make
        the contributions from the two process difficult to separate from the 
        $p_{T}$ distribution alone.
      The mass distribution was used to distinguish between these two processes.
      In turn, the $p_{T}$ becomes dependent on the mass fit. 

      The systematic uncertainty due to the choose of functions used to fit
        the mass distribution was estimated by varying the signal and 
        background functions.
      The contribution to the background from the mass fit was used to fix the
        contribution from the photon-photon process in the $p_{T}$ template
        fit.
      Two functions were used to describe the signal, a Gaussian, and a Crystal
        ball function. 
      The background was fit to a linear function, a 2nd order polynomial, and
        a 2nd order Cheby-Chev polynomial. 
      The resulting variation on the coherent contribution was used to as an
        estimate of this systematic effect. 

      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{ccc}
          \includegraphics[width=.3\textwidth]{cbPolyBkgEst} &
          \includegraphics[width=.3\textwidth]{gausLinBkgEst} &
          \includegraphics[width=.3\textwidth]{gausCCBkgEst} \\
          \includegraphics[width=.3\textwidth]{cbPoly} &
          \includegraphics[width=.3\textwidth]{gausLin} &
          \includegraphics[width=.3\textwidth]{gausCC}
        \end{array} $
        \caption{Various mass distribution fits and the corresponding $p_{T}$
          template fit.}
        \label{fig:massPtFitsForSyst}
      \end{figure}

      Moving from left to right in Fig~\ref{fig:massPtFitsForSyst}, the 
        contribution from the photon process increases.
      The $\chi^{2}$ pre degree of freedom is similar between the three 
        fits indicating a similar goodness of fit.
      On this basis, neither fit is preferred. 
      The left most fit uses a Crystal Ball function to account for the 
        radiative decay of the final state daughters of the J/$\psi$.
      The low mass exponential portion however picks up background events 
        and overestimates the J/$\psi$ contribution. 
      The right most plot fits the background to a 2nd order Cheby-Chev 
        polynomial.
      Because the Cheby-Chev peaks just below the J/$\psi$ peak, this fit 
        overestimates the background and in turn underestimates the signal 
        contribution.
      The Gaussian fit with a linear background however does a reasonable job
        of fitting both the background and the signal. 

      From these three fits an upper and lower bound of the systematics due
        the choice of fit functions was estimated. 
      The difference between the Gaussian-Linear fit and the 
        Crystal Ball-polynomial fit was taken as an upper bound. 
      The difference between the Gaussian-Linear fit and the 
          Gaussian-Cheby-Chev fit was taken as a lower bound. 
      The overall systematic uncertainty due to the choose of mass fit 
        functions is found to be +9.5\% -12\%.

    \subsection{Mass fit}

      \begin{figure}[!Hhtb]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.45\textwidth]{massFitSimple} &
          \includegraphics[width=.45\textwidth]{massFitCBPoly2}
        \end{array} $
        \caption{Mass fit to J/$\psi$ using Gaussian (Left) and Crystal Ball (Right) for the 
          signal and a polynomial for the background}
        \label{fig:massFitSys}
      \end{figure}
      Fig.~\ref{fig:massFitSys} demonstrates the small dependence the raw 
        J/$\psi$ yield has on the fitting function. 
      Both fit functions agree well, with reduced $\chi^{2}$ values below one.
      The Crystal ball fit give an upper estimate for the J/$\psi$ yield.
      The Gaussian fit gives an lower estimate. 
      The main difference comes from the lower mass tails.
      In the Crystal ball fit the lower tail is considered to be signal due to 
        shifting of the mass spectrum to lower mass due to radiation from the 
        final state muons. 
      In the Gaussian fit the lower mass tail is considered to be background and 
        the signal is sharper.

      As check on the simultaneous $p_{T}$ and mass fit, the mass fit is done
        using mass templates from STARlight.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{ptMassSimTemp}
        \caption{Simultaneous fit to the mass and $p_{T}$ using mass templates
          for the mass fit. }
        \label{fig:simFitTemp}
      \end{figure}

    \subsection{MC acceptance}
      The MC derived acceptance correction factors depend on the input physics
        generator. 
      The underlaying $p_{T}$ distribution was assumed to be correctly 
        described by STARlight for the coherent cross section measurement.
      To estimate the effect of changing the underlaying $p_{T}$ distribution 
        on the acceptance measured from the MC, the incoherent sample was used 
        to correct the coherent yield.
      By using the broader $p_{T}$ distribution of the incoherent process, an 
        estimate of acceptance measurements dependence on the assumed shape of
        the $p_{T}$ distribution was obtained.
      The systematic uncertainty due to the dependence of the acceptance 
        correction on the $p_{T}$ distribution of the input physics generator
        was estimated by the difference between the correction factors from 
        the coherent and incoherent MC samples. 
      Half the difference was used as the estimate and was found to be 1.1\%.

      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{coCorInCoAcc}
        \caption{Yields corrected by the MC incoherent acceptance map.}
        \label{fig:coYieldInCoCor}
      \end{figure}

      The effect of polarization was estimated by correcting by the acceptance
        for an unpolarized J/$\psi$ sample.
      \begin{figure}[!Hhtb]
        \centering
        \includegraphics[width=0.6\textwidth]{coCoGaussGun}
        \caption{Yields corrected by an unpolarized J/$\psi$ sample.}
        \label{fig:coYieldGaussCor}
      \end{figure}

    \subsection{ZDC reconstruction}
      An additional method for estimating the ZDC neutron thresholds was used
        to estimate the systematic errors on the threshold measurements.  
      This additional method, used in previous ZDC measurements, differs 
        in the way the signal time slices are used to calculate the signal from
        each channel.
      In the standard method, the signal is taken from the sum of time slices 
        4, 5, and 6.
      To estimate the event by event noise pedestal the sum of time slice 
        1 and 2 are used. 
      The signal for an individual ZDC channel is then calculated as the 
        sum of the signal time slices minus the sum of the noise time slices
        weighted by a factor of 3/2 to account for the differing number of 
        noise versus signal time slices.
      The advantage of the standard method is that by using multiple signal
        and noise time slices the signal and noise are effectively averaged
        reducing time slice to time slice fluctuations.
      However, by using time slices 1 and 2 for measuring the noise, the noise
        can only be measured half the time due to unmeasurable negative 
        fluctuations of the dominant low frequency component of the noise.

      As in the new method described in Section~\ref{sec:breakUpDet}, 
        the standard method combines the channels to create a signal 
        measurement from the whole of each side of the ZDC, one
        measurement for ZDC$^{+}$, and one for ZDC$^{-}$.
      The noise subtracted signal from each of the HAD channels are added 
        together.
      Then the EM section channels are summed. 
      The EM section is weighted by a factor of 0.1 as in the new method. 
      After the weighting the EM and HAD channels are added to each to create
        one measurement for ZDC$^{+}$ and another measurement for ZDC$^{-}$.

      Fig.~\ref{fig:zdcM1Fit} shows the spectra for ZDC$^{+}$ and ZDC${-}$ 
        using the standard method. 
      The same fit used for the new method is applied to standard method. 
      As in the new method, the single neutron threshold is set to 2$\sigma$
        below the mean from the fit to the one neutron peak.
      The multi-neutron threshold was set to 2$\sigma$ above the one neutron
        peak.

      \begin{figure}[!Hhtb]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=0.45\textwidth]{zdcMinusZBFitTimeCut} &
          \includegraphics[width=0.45\textwidth]{zdcPlusZBFitTimeCut}
        \end{array} $
        \caption{Fit to charge spectrum from ZDC$^{-}$ (left) and ZDC$^{+}$ 
          (right) using the standard reconstruction method}
        \label{fig:zdcM1Fit}
      \end{figure}

      The systematic uncertainty due to the ZDC reconstruction method are
        estimated from the difference between the UPC J/$\psi$ candidate yields.
      Both the reconstruction method and thresholds were changed to calculate 
        the effect of the reconstruction method.
      The yields for the new and standard ZDC reconstruction method in the Xn0n
        break up \DIFdelbegin \DIFdel{where }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend found to be 298 and 315 respectively. 
      Half the difference between the two methods was used as an estimated of 
        the systematic uncertainty.
      The systematic uncertainty due to the ZDC reconstruction method was 
        found to be 2.9\%.

    \subsection{ZDC reconstruction method comparison}
      The new method relative to the standard method separates low signal from 
        the noise more effectively for both sides of the ZDC.
      This is particularly important for ZDC$^{+}$ where the 1st HAD section
        had a lower gain than the other sections. 
      The ZDC$^{+}$ and ZDC$^{-}$ signals near the one neutron peak using the
        standard and new reconstruction methods were plotted for comparison in 
        Fig.~\ref{fig:zdcSpec2v1}.
      \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{zdcSpec2v1}
        \caption{Comparison of the \textcolor{red}{new} ZDC reconstruction 
          method and the \textcolor{blue}{standard} method for ZDC$^{-}$ (left) and 
          ZDC$^{+}$ (right).}
        \label{fig:zdcSpec2v1}
      \end{figure}
      In Fig.~\ref{fig:zdcSpec2v1}, the shrinking of width of the noise peak 
        around zero in the new method versus the old method is apparent for
        both ZDC$^{+}$ and ZDC$^{-}$.
      For the standard method no single neutron peak is resolved in ZDC$^{+}$,
        whereas the single neutron peak is resolved using the new method. 

      Timing cuts were applied to enhance the signal relative to the background
        in order to resolve the one neutron peak in ZDC$^{+}$ using the 
        standard method. 
      Because the products of the collision are synced with time slice 4, noise
        can be rejected by selecting channels where the maximum signal falls 
        into time slice 4.
      The noise will have no preferred time slice (see Fig.~\ref{fig:zdcPulseShape}). 
      Using this fact, signal can be preferably selected by requiring that the
        hadronic channels of the ZDC have a peak signal in the fourth time 
        slice.
      Through these timing cuts the single neutron peak was recovered using the
       standard reconstruction for ZDC$^{+}$.

      To examine the effectiveness of the timing cuts, event by event noise 
        subtraction was removed from the standard reconstruction.
      The signal from each channel was taken from time slices 4,5, and 6 with
        out subtracting 1 and 2.
      The signal spectrum from ZDC$^{-}$ was then plotted with the result
        shown in Fig.~\ref{fig:zdcTimingCuts}.
      \begin{figure}[!Hhbt]
        \centering
        \includegraphics[width=0.6\textwidth]{zdcMinusSingleNuNoInc}
        \caption{Effects of requiring in-time signal in successively more 
          ZDC hadronic channels, no timing, at least \textcolor{red}{one}, at least \textcolor{green}{two},
            at least \textcolor{blue}{three}, and all \textcolor{yellow}{four} HAD channels have a maximum signal
            in the fourth time slice.}
        \label{fig:zdcTimingCuts}
      \end{figure}
      As each additional hadronic channel is required to have a maximum signal
        in the fourth time slice, the single neutron peak emerges. 
      Fig.~\ref{fig:zdcTimingCuts} demonstrates that the single neutron peak 
        can be recovered from the noise using timing cuts alone. 

      Using the standard noise subtraction method, the same signal that emerges
        from the timing cuts alone appear without timing cuts.
       \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\textwidth]{zdcMinusSingleNuNoSub}
        \caption{Effect of ZDC signal timing requirements after noise 
          subtraction.}
        \label{fig:zdcTimingAfterNoiseSub}
      \end{figure}
      Fig.~\ref{fig:zdcTimingAfterNoiseSub} confirms that both noise 
        subtraction and the timing requirement produce the same signal.
      This gives confidence that the signal is not an artifact of either cut, 
        but the true neutron signal.

       Fig.~\ref{fig:zdcTimingAfterNoiseSub} and Fig.~\ref{fig:zdcSpec2v1} 
        demonstrate the consistence of the using timing cuts and noise 
        subtraction to enhance the signal neutron peak. 
      Fig.~\ref{fig:zdcTimingAfterNoiseSub} confirms the legitimacy of the 
        timing requirement method in ZDC$^{-}$ by showing the that the same
        signal emerges from the noise subtraction method as the timing method.
      Fig.~\ref{fig:zdcSpec2v1} demonstrates the corresponds between
        the new noise subtraction method and the standard method on in 
        ZDC$^{-}$ where signal is better separated from the electronic noise. 
      This allows for confidence that the signal seen in ZDC$^{+}$ using 
        the new method is the one neutron peak.

    \subsection{ZDC trigger efficiency}
      The ZDC trigger efficiency measurement is sensitive to the underlying 
        neutron distribution.
      The more neutrons that high the ZDC the higher the trigger efficiency 
        will be.
      To estimate the effect the input sample has on the efficiency, the ZDC 
        trigger efficiency was measured from five different samples.
      The Table~\ref{tab:zdcEfficiencySys} shows the results from the 
       five samples. 
      Both the new and standard ZDC reconstruction methods are shown for 
        comparison.

      \begin{table}
        \centering
        \begin{tabular}{|c|c|c|c|c|}
          \hline ZDC Side & Reco Method & N$_{events}$ & N$_{trig}$ & $\varepsilon_{ZDC}$ \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{+}$ or ZDC$^{-}$) and 1 pixel track} \\ \hline 
           ZDC$^{-}$ & 1 & 72946  & 71688 & 0.982 $\pm$ 0.005 \\ \hline
           ZDC$^{-}$ & 2 & 73028  & 71706  & 0.9819  $\pm$ 0.005  \\ \hline
           ZDC$^{+}$ & 1 & 76137  & 71786  & 0.9429  $\pm$ 0.005  \\ \hline
           ZDC$^{+}$ & 2 & 76132  & 71859  & 0.9439  $\pm$ 0.005  \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{-}$ or ZDC$^{+}$), 1 pixel track, and L1 EG trigger } \\ \hline 
           ZDC$^{-}$ & 1 & 613758  & 602123  & 0.9810 $\pm$ 0.0018 \\ \hline
           ZDC$^{-}$ & 2 & 614014  & 601863  & 0.9802 $\pm$ 0.0018 \\ \hline
           ZDC$^{+}$ & 1 & 643905  & 602671  & 0.9360  $\pm$ 0.0017 \\ \hline
           ZDC$^{+}$ & 2 & 647888  & 603089  & 0.9309  $\pm$ 0.0017 \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{-}$ or ZDC$^{+}$), 1 pixel track, and L1 Muon trigger} \\ \hline 
           ZDC$^{-}$ & 1 & 65466  & 63376  & 0.9681 $\pm$ 0.0054  \\ \hline
           ZDC$^{-}$ & 2 & 65543  & 63358  & 0.9667 $\pm$ 0.0054 \\ \hline
           ZDC$^{+}$ & 1 & 71929  & 63512  & 0.8830  $\pm$ 0.0048 \\ \hline
           ZDC$^{+}$ & 2 & 72932  & 63582  & 0.8718  $\pm$ 0.0047 \\ \hline
           \multicolumn{5}{|c|}{ Zero Bias with ZDC timing cuts} \\ \hline 
           ZDC$^{-}$ & 1 & 88676  & 84429  & 0.9521 $\pm$ 0.0046 \\ \hline
           ZDC$^{-}$ & 2 & 88480  & 84202  & 0.9517 $\pm$ 0.0046 \\ \hline
           ZDC$^{+}$ & 1 & 59878  & 54728  & 0.9140  $\pm$ 0.0054 \\ \hline
           ZDC$^{+}$ & 2 & 60467  & 54733  & 0.9052  $\pm$ 0.0053 \\ \hline
           \multicolumn{5}{|c|}{(ZDC$^{-}$ or ZDC$^{+}$)} \\ \hline 
           ZDC$^{-}$ & 1 & 30986 & 30333 & 0.9789 $\pm$ 0.0079 \\ \hline
           ZDC$^{-}$ & 2 & 31029 & 30339 & 0.9778 $\pm$ 0.0079 \\ \hline
           ZDC$^{+}$ & 1 & 39178 & 30164 & 0.7699 $\pm$ 0.0059 \\ \hline
           ZDC$^{+}$ & 2 & 35703 & 30443 & 0.8527 $\pm$ 0.0067 \\ \hline
           \multicolumn{5}{|c|}{ Zero Bias} \\ \hline 
           ZDC$^{-}$ & 1 & 109967  & 101598  & 0.9239 $\pm$ 0.0040 \\ \hline
           ZDC$^{-}$ & 2 & 110230  & 101561  & 0.9214 $\pm$ 0.0040 \\ \hline
           ZDC$^{+}$ & 1 & 253241  & 86660  & 0.3422 $\pm$ 0.0013 \\ \hline
           ZDC$^{+}$ & 2 & 156336  & 87401  & 0.5591 $\pm$ 0.0024 \\ \hline
        \end{tabular}
        \caption{ZDC trigger efficiencies for ZDC reconstruction method 1 and 
          2 for different trigger samples}
        \label{tab:zdcEfficiencySys}
      \end{table}

      The amount of electronic noise in the sample also effects the measurement.
      The more noise sits below the one neutron peak, the worse the efficiency 
        is. 
      In Table~\ref{tab:zdcEfficiencySys}, the Zero Bias sample compared the 
        Zero Bias sample with the timing cuts the described in the previous 
        section shows a significant increase in efficiency in the sample
        with reduced noise. 
      The same increase is seen when comparing the ZDC triggered sample with 
        the ZDC triggered sample that also requires a pixel track. 
      The effect of the electronic noise is also present in the difference seen
        in using the two methods.
      As seen in Fig.~\ref{fig:zdcSpec2v1}, the new reconstruction method 
        shows better separation of the one neutron peak from the electronic 
        noise, in particular in ZDC$^{+}$ where the signal gain is lower.
      For this reason, the Zero Bias data, which contains that largest 
        contribution from electronic noise, shows the most separation between 
        the two methods and give the lowest estimate for the ZDC trigger 
        efficiency.

      The systematic uncertainty due to the uncertainty in the 
        underlying distribution was estimated by calculating the standard 
        deviation of the least extreme values from 
        Table~\ref{tab:zdcEfficiencySys}.
      Any value greater than three standard deviations from the mean was thrown
        out. 

    \subsection{Tag and probe}
      The main purpose for fitting the mass spectra to estimate the efficiency
        is to separate the background from true signal. 
      The background may not have the same efficiency as the signal, so 
        separating the two is important if this is the case.
      In the tag and probe fit the signal peak from the J/$\psi$ resonance
        is fit to the probes, passing probes, and failing probes alike (see
        Fig.~\ref{fig:tnpFitPlot}). 
      The signal shape, if from the same physical signal, will be 
        identical in each of the three distributions. 
      The background is for the passing and failing probes is fit using 
        different parameters for the background because the background
        may come from different physical processes than the signal or 
        non-physical sources like combinatorial backgrounds or misidentified
        fake particles.
      When the background comes from sources other than the physical signal,
        the background may give an efficiency estimate that is lower than
        the signal. 

      The trigger efficiency measured by the tag and probe method depend on
        the fitting functions use to estimate the background and signal 
        contributions. 
      Depending on what functions is used to fit the spectra, the amount of
        amount of background can be over or underestimated and effect the 
        efficiency measurement.
      To estimate this effect, the tag and probe efficiencies were additionally
        measured by counting probes in the J/$\psi$ mass window. 
      The whole mass window is used to estimate the efficiency including all 
        the events from the mass side bands.
      In this way, a worst case scenario estimate is given where all background
        events are included as signal. 
      \begin{figure}[!Hhbt]
        \centering
        $ \begin{array}{cc}
          \includegraphics[width=.45\textwidth]{tNp/tnpCounting} &
          \includegraphics[width=.45\textwidth]{tNp/tnpFromFit}
        \end{array} $ 
        \caption{Tag and probe trigger efficiencies from counting (left) 
          compared to fitting (right)}
        \label{fig:tnpCntVFit}
      \end{figure}

      From Fig.~\ref{fig:tnpCntVFit} it is apparent that the choice of fit 
        function and therefore the amount of background from the mass side 
        bands is included in the signal measurement has very little effect on 
        the tag and probe efficiency measurement.
      The small effect of including the side bands is due to the side bands 
        being comprised mostly of photon-photon events.
      Because this background is neither decays from other particles like pions
        nor is it non-physical background like combinatorics, the efficiency
        for muons from the sidebands are nearly identical to J/$\psi$ signal.
      The photon-photon process directly produces two muons just like the 
        J/$\psi$, therefore efficiency estimated from the side bands has 
        little effect on the measurement because of this similarity.
      The counting and fitting trigger efficiency measurements agree within 
        statistcal uncertainties, so this uncertianty was taken to be negliable.

    \subsection{MC vs Data compairson}
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{jpsiMcComp/jpsiAbsRapCoherent}
        \caption{Comparison of the of the dimuon rapidity distributions between 
          coherent MC sample and Data.}
        \label{fig:jpsiAbsRapCoherent}
      \end{figure}
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{jpsiMcComp/jpsiPhiCoherent}
        \caption{Comparison of the of the dimuon $\varphi$ distributions 
          between coherent MC sample and Data.}
        \label{fig:jpsiPhiCoherent}
      \end{figure}
      \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{jpsiMcComp/jpsiPtCoherent}
        \caption{Comparison of the of the dimuon $p_{T}$ distributions 
          between coherent MC sample and Data.}
        \label{fig:jpsiPtCoherent}
      \end{figure}

